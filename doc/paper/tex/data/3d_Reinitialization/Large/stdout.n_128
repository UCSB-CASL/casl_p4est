Tue Feb 4 16:29:16 CST 2014
TACC: Starting up job 2716452
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[WARNING]: option 'log_summary' does not exists in the database -- ignoring.
 -------------------== CASL Options Database ==------------------- 
 List of entered options:

  -enable-qnnn-buffer no-arg
  -lmax 10
  -lmin 0
  -output-dir /scratch/02032/mmirzade/3d_reinit_s_buff/n_128.N_8.2716452
  -test strong
 ----------------------------------------------------------------- 
git commit hash value = c05f9ad (c05f9adbc735149569977450adba3bc430802ba5)
total time ... 
connectivity ... 
connectivity ... done in 
 0.00029 secs. on process 0 [Note: only showing root's timings]

p4est generation ... 
p4est generation ... done in 
 0.03513 secs. on process 0 [Note: only showing root's timings]

refine ... 
refine ... done in 
 3.96117 secs. on process 0 [Note: only showing root's timings]

partition ... 
partition ... done in 
 1.13828 secs. on process 0 [Note: only showing root's timings]

gather statistics ... 
% global_quads = 285910036 	 global_nodes = 337372434
% mpi_rank local_node_size local_quad_size ghost_node_size ghost_quad_size
   0, 2610311, 2233672, 79332, 31978
   1, 2639394, 2233672, 63557, 32747
   2, 2638854, 2233672, 67147, 37175
   3, 2649179, 2233672, 50969, 36881
   4, 2646808, 2233672, 61248, 36508
   5, 2630780, 2233672, 82189, 39294
   6, 2620854, 2233673, 116986, 54601
   7, 2640302, 2233672, 77912, 46045
   8, 2632676, 2233672, 81802, 40297
   9, 2637700, 2233672, 81368, 42821
  10, 2643677, 2233672, 97090, 54896
  11, 2647391, 2233672, 78137, 47830
  12, 2646937, 2233673, 78093, 48573
  13, 2635588, 2233672, 97795, 53212
  14, 2657543, 2233672, 48678, 35897
  15, 2612638, 2233672, 114069, 47749
  16, 2637142, 2233672, 100588, 55673
  17, 2633740, 2233672, 120642, 64559
  18, 2627439, 2233672, 132522, 64291
  19, 2661058, 2233673, 99790, 69107
  20, 2627905, 2233672, 64657, 28533
  21, 2634433, 2233672, 92470, 47267
  22, 2640192, 2233672, 82530, 44473
  23, 2635166, 2233672, 89653, 46682
  24, 2657492, 2233672, 108413, 68347
  25, 2637488, 2233673, 77187, 38647
  26, 2634796, 2233672, 82528, 41875
  27, 2643406, 2233672, 65412, 39381
  28, 2649148, 2233672, 68184, 42267
  29, 2613794, 2233672, 118922, 50914
  30, 2618418, 2233672, 154554, 76438
  31, 2665111, 2233673, 120440, 79327
  32, 2640797, 2233672, 91736, 51877
  33, 2650101, 2233672, 76973, 47568
  34, 2611166, 2233672, 161523, 77517
  35, 2648773, 2233672, 58395, 36404
  36, 2624851, 2233672, 86058, 38836
  37, 2642400, 2233672, 62478, 35480
  38, 2634725, 2233673, 74950, 41421
  39, 2640473, 2233672, 104364, 56523
  40, 2626799, 2233672, 94779, 45407
  41, 2639077, 2233672, 91173, 50031
  42, 2627291, 2233672, 113653, 57362
  43, 2634354, 2233672, 94068, 50968
  44, 2644584, 2233673, 98267, 55094
  45, 2621610, 2233672, 84235, 37126
  46, 2616701, 2233672, 142200, 62529
  47, 2636665, 2233672, 87818, 51155
  48, 2638079, 2233672, 75099, 38551
  49, 2619385, 2233672, 85029, 40523
  50, 2643864, 2233672, 101418, 59458
  51, 2636373, 2233673, 97519, 50526
  52, 2627170, 2233672, 68885, 35780
  53, 2630530, 2233672, 68181, 33639
  54, 2640339, 2233672, 100694, 53478
  55, 2641504, 2233672, 90129, 51475
  56, 2629594, 2233672, 79683, 37413
  57, 2647411, 2233673, 69462, 40656
  58, 2644485, 2233672, 37895, 24249
  59, 2635009, 2233672, 83077, 42687
  60, 2639353, 2233672, 81879, 47065
  61, 2649222, 2233672, 47638, 31582
  62, 2624342, 2233672, 79586, 34103
  63, 2629333, 2233673, 77103, 36568
  64, 2653318, 2233672, 77436, 49582
  65, 2644621, 2233672, 77550, 46459
  66, 2635900, 2233672, 84521, 43161
  67, 2624329, 2233672, 110851, 49496
  68, 2631813, 2233672, 84313, 43688
  69, 2629284, 2233672, 116002, 60595
  70, 2635852, 2233673, 70827, 40206
  71, 2628445, 2233672, 112636, 55094
  72, 2641913, 2233672, 119598, 64042
  73, 2644925, 2233672, 76018, 41557
  74, 2638835, 2233672, 90831, 52240
  75, 2632407, 2233672, 97877, 49882
  76, 2622104, 2233673, 100771, 47570
  77, 2633340, 2233672, 89690, 44656
  78, 2625493, 2233672, 80420, 36397
  79, 2639253, 2233672, 100372, 53402
  80, 2647126, 2233672, 101165, 56252
  81, 2625890, 2233672, 76781, 34863
  82, 2626503, 2233672, 90738, 46369
  83, 2627422, 2233673, 115838, 63690
  84, 2642301, 2233672, 72012, 41851
  85, 2651800, 2233672, 85154, 54058
  86, 2655221, 2233672, 74916, 45159
  87, 2618831, 2233672, 107226, 47837
  88, 2624619, 2233672, 66648, 29334
  89, 2637538, 2233673, 85126, 43806
  90, 2628171, 2233672, 96984, 46957
  91, 2642839, 2233672, 59941, 34221
  92, 2637088, 2233672, 72081, 37158
  93, 2625590, 2233672, 80963, 38158
  94, 2627298, 2233672, 113018, 56895
  95, 2640930, 2233673, 66941, 35842
  96, 2625718, 2233672, 113117, 56271
  97, 2645308, 2233672, 94165, 49968
  98, 2647430, 2233672, 80534, 48027
  99, 2646565, 2233672, 72190, 44128
 100, 2633623, 2233672, 102620, 51564
 101, 2621701, 2233672, 123511, 55982
 102, 2638166, 2233673, 66645, 37237
 103, 2622972, 2233672, 104115, 48977
 104, 2639276, 2233672, 71234, 40961
 105, 2631146, 2233672, 109259, 53557
 106, 2649077, 2233672, 70716, 45016
 107, 2635939, 2233672, 93683, 46388
 108, 2627167, 2233673, 82156, 38201
 109, 2647995, 2233672, 63734, 39701
 110, 2614255, 2233672, 139967, 68080
 111, 2617101, 2233672, 109689, 50619
 112, 2615538, 2233672, 109993, 48826
 113, 2685054, 2233672, 113220, 81075
 114, 2632590, 2233672, 68867, 37409
 115, 2615945, 2233673, 110316, 48831
 116, 2635029, 2233672, 74143, 40814
 117, 2622737, 2233672, 87859, 38153
 118, 2637615, 2233672, 79465, 38832
 119, 2633175, 2233672, 80515, 38417
 120, 2626565, 2233672, 85687, 42296
 121, 2649822, 2233673, 85468, 44928
 122, 2629261, 2233672, 124228, 61541
 123, 2629026, 2233672, 67479, 30763
 124, 2632786, 2233672, 98406, 47951
 125, 2642256, 2233672, 72811, 36179
 126, 2640944, 2233672, 59105, 32825
 127, 2655933, 2233673, 39173, 31231
gather statistics ... done in 
 0.00552 secs. on process 0 [Note: only showing root's timings]

Reinit_1st_2nd ... 
Reinit_1st_2nd ... done in 
 14.15856 secs. on process 0 [Note: only showing root's timings]

Reinit_2nd_2nd ... 
Reinit_2nd_2nd ... done in 
 28.75978 secs. on process 0 [Note: only showing root's timings]

total time ... done in 
 64.50521 secs. on process 0 [Note: only showing root's timings]

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./reinitialization_3d.prof on a sandybridge-cxx named c415-001.stampede.tacc.utexas.edu with 128 processors, by mmirzade Tue Feb  4 16:30:30 2014
Using Petsc Release Version 3.4.3, Oct, 15, 2013 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.489e+01      1.00001   6.489e+01
Objects:              9.700e+01      1.00000   9.700e+01
Flops:                0.000e+00      0.00000   0.000e+00  0.000e+00
Flops/sec:            0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Messages:         8.482e+03     10.57606   2.945e+03  3.770e+05
MPI Message Lengths:  1.652e+08      3.17231   3.298e+04  1.243e+10
MPI Reductions:       1.300e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.4894e+01 100.0%  0.0000e+00   0.0%  3.770e+05 100.0%  3.298e+04      100.0%  1.290e+02  99.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

PetscBarrier          32 1.0 1.2191e+01 1.1 0.00e+00 0.0 2.3e+05 3.9e+04 0.0e+00 18  0 60 70  0  18  0 60 70  0     0
my_p4est_level_set::reinit_2nd_order                           1 1.0 2.8998e+01 1.0 0.00e+00 0.0 2.3e+05 3.4e+04 6.4e+01 44  0 62 63 49  44  0 62 63 50     0
my_p4est_level_set::reinit_1st_time_2nd_space                  1 1.0 1.4287e+01 1.0 0.00e+00 0.0 1.3e+05 3.1e+04 5.6e+01 22  0 36 34 43  22  0 36 34 43     0
my_p4est_level_set::reinit_1_iter_2nd_order                   60 1.0 3.1096e+01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 45  0  0  0  0  45  0  0  0  0     0
my_p4est_hierarchy_t::init                                     1 1.0 3.5684e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
my_p4est_node_neighbors_t::init                                1 1.0 2.7480e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
my_p4est_node_neighbors_t::2nd_derivatives_cent               32 1.0 1.2191e+01 1.1 0.00e+00 0.0 2.3e+05 3.9e+04 0.0e+00 18  0 60 70  0  18  0 60 70  0     0
my_p4est_nodes_new                                             1 1.0 1.0898e+01 1.0 0.00e+00 0.0 4.7e+03 6.1e+04 1.0e+00 17  0  1  2  1  17  0  1  2  1     0
my_p4est_new                                                   1 1.0 3.7900e-02 101.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
my_p4est_ghost_new                                             1 1.0 5.1566e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
my_p4est_refine                                               10 1.0 2.4255e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
my_p4est_partition                                            11 1.0 2.6742e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecScatterBegin      126 1.0 1.2202e-01 3.8 0.00e+00 0.0 3.0e+05 3.9e+04 0.0e+00  0  0 79 92  0   0  0 79 92  0     0
VecScatterEnd        126 1.0 3.1732e+0020.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    32             32    344322688     0
      Vector Scatter    16             16        16832     0
           Index Set    32             32      5101440     0
   IS L to G Mapping    16             16         9280     0
              Viewer     1              0            0     0
========================================================================================================================
Average time to get PetscTime(): 0
Average time for MPI_Barrier(): 1.50204e-05
Average time for zero size MPI_Send(): 3.57814e-06
#PETSc Option Table entries:
-enable-qnnn-buffer
-lmax 10
-lmin 0
-log_summary
-output-dir /scratch/02032/mmirzade/3d_reinit_s_buff/n_128.N_8.2716452
-test strong
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Thu Dec 12 09:53:13 2013
Configure options: --with-x=0 -with-pic --with-external-packages-dir=/opt/apps/intel13/mvapich2_1_9/petsc/3.4/externalpackages --with-mpi-compilers=1 --with-mpi-dir=/opt/apps/intel13/mvapich2/1.9 --with-clanguage=C++ --with-scalar-type=real --with-dynamic-loading=0 --with-shared-libraries=1 --with-spai=1 --download-spai --with-hypre=1 --download-hypre --with-mumps=1 --download-mumps --with-scalapack=1 --download-scalapack --with-blacs=1 --download-blacs --with-spooles=1 --download-spooles --with-superlu=1 --download-superlu --with-superlu_dist=1 --download-superlu_dist --with-parmetis=1 --download-parmetis --with-metis=1 --download-metis --with-hdf5=1 --with-hdf5-dir=/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9 --with-debugging=no --with-blas-lapack-dir=/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 --with-mpiexec=mpirun_rsh --COPTFLAGS= --CXXOPTFLAGS= --FOPTFLAGS=
-----------------------------------------
Libraries compiled on Thu Dec 12 09:53:13 2013 on build.stampede.tacc.utexas.edu 
Machine characteristics: Linux-2.6.32-358.18.1.el6.x86_64-x86_64-with-centos-6.4-Final
Using PETSc directory: /opt/apps/intel13/mvapich2_1_9/petsc/3.4
Using PETSc arch: sandybridge-cxx
-----------------------------------------

Using C compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx  -wd1572    -fPIC   ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpif90  -fPIC    ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/include -I/opt/apps/intel13/mvapich2/1.9/include
-----------------------------------------

Using C linker: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx
Using Fortran linker: /opt/apps/intel13/mvapich2/1.9/bin/mpif90
Using libraries: -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lpetsc -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lsuperlu_4.3 -lHYPRE -lspai -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -lpthread -lparmetis -lmetis -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -L/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -lhdf5_fortran -lhdf5_hl -lhdf5 -Wl,-rpath,/opt/ofed/lib64 -L/opt/ofed/lib64 -Wl,-rpath,/opt/apps/limic2/0.5.5/lib -L/opt/apps/limic2/0.5.5/lib -Wl,-rpath,/opt/apps/intel13/mvapich2/1.9/lib -L/opt/apps/intel13/mvapich2/1.9/lib -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -lmpichf90 -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.2.146/compiler/lib/intel64 -lifport -lifcore -lm -lm -lmpichcxx -ldl -lmpich -lopa -lmpl -libmad -lrdmacm -libumad -libverbs -lrt -llimic2 -lpthread -limf -lsvml -lirng -lipgo -ldecimal -lcilkrts -lstdc++ -lgcc_s -lirc -lirc_s -ldl 
-----------------------------------------

##IPMv0.983####################################################################
# 
# command : ./reinitialization_3d.prof -lmin 0 -lmax 10 -test strong -output-dir /scratch/02032/mmirzade/3d_reinit_s_buff/n_128.N_8.2716452 -log_summary -enable-qnnn-buffer  (completed)
# host    : c415-001/x86_64_Linux          mpi_tasks : 128 on 8 nodes
# start   : 02/04/14/16:29:25              wallclock : 64.997298 sec
# stop    : 02/04/14/16:30:30              %comm     : 11.00 
# gbytes  : 2.38720e+02 total              gflop/sec : NA 
#
##############################################################################
# region  : *       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                        128             1             1             1
# wallclock                  8319.57       64.9967       64.9962       64.9973
# user                        8191.2       63.9938       62.6735       64.2382
# system                     169.842       1.32689       1.11983       2.44563
# mpi                        915.506       7.15239       4.80797       8.00404
# %comm                                    11.0041       7.39724       12.3145
# gbytes                      238.72         1.865       1.83814       1.96632
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                  316.759          5602         34.60         3.81
# MPI_Waitany                 292.95        334126         32.00         3.52
# MPI_Allgather              135.689          6912         14.82         1.63
# MPI_Recv                   112.942          6238         12.34         1.36
# MPI_Allreduce              31.6392        147968          3.46         0.38
# MPI_Waitall                18.8249         23680          2.06         0.23
# MPI_Start                  2.32406        296478          0.25         0.03
# MPI_Gather                  1.6807          2816          0.18         0.02
# MPI_Barrier                1.43795          1024          0.16         0.02
# MPI_Isend                 0.646382         86921          0.07         0.01
# MPI_Startall              0.386998         16128          0.04         0.00
# MPI_Bcast                  0.12833           768          0.01         0.00
# MPI_Irecv                0.0597445         81319          0.01         0.00
# MPI_Recv_init            0.0182187         75296          0.00         0.00
# MPI_Send_init            0.0122807         75296          0.00         0.00
# MPI_Comm_size           0.00277942         23808          0.00         0.00
# MPI_Comm_rank           0.00261075         30720          0.00         0.00
# MPI_Send                0.00144202           636          0.00         0.00
# MPI_Wait               0.000509738           896          0.00         0.00
##############################################################################
# region  : reinit_2nd_2nd       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       2560            20            20            20
# wallclock                  2635.74       20.5917        19.954       21.0925
# user                       2627.91       20.5306        19.878       21.0728
# system                     5.00825     0.0391269             0      0.661902
# mpi                        123.228      0.962718    0.00725964       1.74801
# %comm                                    4.56427      0.034521       8.28736
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                122.752         47060         99.61         4.66
# MPI_Waitall               0.249035          2560          0.20         0.01
# MPI_Start                 0.183188         47060          0.15         0.01
# MPI_Startall              0.043696          2560          0.04         0.00
##############################################################################
# region  : 2nd_derivatives       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       4096            32            32            32
# wallclock                  1480.28       11.5647       10.9907       12.1859
# user                       1472.34       11.5026       10.7754       12.1572
# system                     5.71313     0.0446338         0.006      0.388945
# mpi                        145.407       1.13599       0.10569       2.04956
# %comm                                    9.32219      0.943713       16.8192
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                141.083        225888         97.03         9.53
# MPI_Start                  2.03519        225888          1.40         0.14
# MPI_Waitall                1.96195         12288          1.35         0.13
# MPI_Startall              0.326796         12288          0.22         0.02
##############################################################################
# region  : p4est_nodes_new       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                        128             1             1             1
# wallclock                  1387.32       10.8385       10.8306       10.8934
# user                        1364.2       10.6578       10.6164       10.7214
# system                     21.9617      0.171575      0.141978      0.210968
# mpi                        527.602       4.12189       3.89305       4.25005
# %comm                                    37.8385       35.9342         39.23
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                  316.759          5602         60.04        22.83
# MPI_Recv                     112.6          5602         21.34         8.12
# MPI_Allgather              94.6605           128         17.94         6.82
# MPI_Allreduce              3.34465           256          0.63         0.24
# MPI_Waitall                0.15859           128          0.03         0.01
# MPI_Isend                0.0786498          5602          0.01         0.01
# MPI_Wait               0.000509738           896          0.00         0.00
# MPI_Comm_size          8.71085e-05           128          0.00         0.00
# MPI_Comm_rank          5.98347e-05           128          0.00         0.00
##############################################################################
# region  : reinit_1st_2nd       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       1280            10            10            10
# wallclock                  1284.94       10.0386       9.86332       10.2099
# user                        1283.3       10.0258        9.8445       10.1984
# system                    0.644901    0.00503829             0      0.055991
# mpi                        28.0423      0.219081    0.00316489       0.46715
# %comm                                    2.14577     0.0319374       4.57547
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                27.8636         23530         99.36         2.17
# MPI_Start                 0.105691         23530          0.38         0.01
# MPI_Waitall              0.0565046          1280          0.20         0.00
# MPI_Startall             0.0165059          1280          0.06         0.00
##############################################################################
# region  : ipm_noregion       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                      11264            88            88            88
# wallclock                  807.645       6.30973       6.10861       6.47132
# user                        750.21       5.86101       5.59715       6.03208
# system                       110.3       0.86172      0.761883       1.05284
# mpi                        51.0879      0.399124      0.261374       0.50729
# %comm                                    6.16758       4.16375       7.84907
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather              28.2766          4096         55.35         3.50
# MPI_Allreduce              18.9262        139392         37.05         2.34
# MPI_Barrier                1.43795          1024          2.81         0.18
# MPI_Waitany                1.25128         37648          2.45         0.15
# MPI_Waitall               0.383502          4096          0.75         0.05
# MPI_Recv                  0.341955           636          0.67         0.04
# MPI_Isend                 0.284365         75296          0.56         0.04
# MPI_Bcast                  0.12833           768          0.25         0.02
# MPI_Irecv                0.0205682         75296          0.04         0.00
# MPI_Recv_init            0.0182187         75296          0.04         0.00
# MPI_Send_init            0.0122807         75296          0.02         0.00
# MPI_Comm_size           0.00267912         23552          0.01         0.00
# MPI_Comm_rank           0.00254235         30464          0.00         0.00
# MPI_Send                0.00144202           636          0.00         0.00
##############################################################################
# region  : p4est_partition       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       1408            11            11            11
# wallclock                  342.155       2.67309       2.67289       2.67315
# user                       323.691       2.52883       2.49462        2.6106
# system                     17.9583      0.140299      0.061991      0.172972
# mpi                        15.4296      0.120544     0.0712385      0.248578
# %comm                                    4.50944       2.66498       9.29928
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather              7.09313          1408         45.97         2.07
# MPI_Allreduce              5.27419          5632         34.18         1.54
# MPI_Gather                  1.6807          2816         10.89         0.49
# MPI_Waitall                1.35428          2816          8.78         0.40
# MPI_Isend                0.0255924          1351          0.17         0.01
# MPI_Irecv               0.00173102          1351          0.01         0.00
##############################################################################
# region  : p4est_refine       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       1280            10            10            10
# wallclock                  310.324        2.4244       2.42417       2.42453
# user                       302.291       2.36165       2.33964       2.37964
# system                     7.88879     0.0616311      0.042994      0.078988
# mpi                        6.84504     0.0534769     0.0213675     0.0874675
# %comm                                    2.20566      0.881379       3.60789
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather              5.65862          1280         82.67         1.82
# MPI_Allreduce              1.18643          2560         17.33         0.38
##############################################################################
# region  : p4est_ghost_new       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                        128             1             1             1
# wallclock                  64.9717      0.507592      0.452676       0.51545
# user                       64.3522      0.502752      0.447931      0.511923
# system                    0.297955    0.00232777             0      0.008999
# mpi                        14.9563      0.116846    0.00147609      0.231643
# %comm                                    22.6688      0.287659       45.0923
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitall                14.6611           512         98.03        22.57
# MPI_Isend                 0.257774          4672          1.72         0.40
# MPI_Irecv                0.0374453          4672          0.25         0.06
##############################################################################
# region  : p4est_new       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                        128             1             1             1
# wallclock                  2.96461     0.0231611    0.00020922     0.0376679
# user                       2.88656     0.0225513             0      0.036995
# system                    0.042994   0.000335891             0         0.002
# mpi                        2.90775     0.0227168   1.45677e-05     0.0374459
# %comm                                    60.3082       6.96289       99.4898
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce              2.90773           128        100.00        98.08
# MPI_Comm_size          1.31885e-05           128          0.00         0.00
# MPI_Comm_rank          8.56724e-06           128          0.00         0.00
###############################################################################
 
TACC: Shutdown complete. Exiting.
