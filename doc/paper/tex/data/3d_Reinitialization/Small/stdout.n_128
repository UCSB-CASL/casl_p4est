Sat Feb 15 13:56:47 CST 2014
TACC: Starting up job 2807901
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[WARNING]: option 'log_summary' does not exists in the database -- ignoring.
 -------------------== CASL Options Database ==------------------- 
 List of entered options:

  -enable-qnnn-buffer no-arg
  -lmax 8
  -lmin 0
  -output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_128.N_8.2807901
  -test strong
 ----------------------------------------------------------------- 
git commit hash value = c05f9ad (c05f9adbc735149569977450adba3bc430802ba5)
total time ... 
connectivity ... 
connectivity ... done in 
 0.00028 secs. on process 0 [Note: only showing root's timings]

p4est generation ... 
p4est generation ... done in 
 0.03270 secs. on process 0 [Note: only showing root's timings]

refine ... 
refine ... done in 
 0.24746 secs. on process 0 [Note: only showing root's timings]

partition ... 
partition ... done in 
 0.06381 secs. on process 0 [Note: only showing root's timings]

gather statistics ... 
% global_quads = 17872420 	 global_nodes = 21114960
% mpi_rank local_node_size local_quad_size ghost_node_size ghost_quad_size
   0,  160483,  139628, 18232,  7498
   1,  164814,  139628, 16829,  8443
   2,  165812,  139628, 15985,  8697
   3,  169499,  139629, 13098,  9379
   4,  167675,  139628, 16857,  9634
   5,  163905,  139628, 22840, 11077
   6,  161849,  139628, 27440, 12848
   7,  165926,  139629, 18736, 11125
   8,  163311,  139628, 19871,  9628
   9,  164320,  139628, 19938, 10435
  10,  166869,  139629, 21190, 12344
  11,  167444,  139628, 18921, 11236
  12,  166787,  139628, 18191, 11238
  13,  165914,  139628, 27283, 14591
  14,  170549,  139629, 13024,  9637
  15,  159527,  139628, 26894, 11318
  16,  165578,  139628, 24728, 13722
  17,  164693,  139629, 28513, 15233
  18,  161080,  139628, 28958, 14200
  19,  172352,  139628, 24631, 16731
  20,  162575,  139628, 15701,  6872
  21,  165432,  139629, 22320, 11429
  22,  165377,  139628, 20429, 10809
  23,  165275,  139628, 21057, 11246
  24,  169961,  139629, 25464, 15739
  25,  163849,  139628, 19652,  9515
  26,  164471,  139628, 20891, 10510
  27,  165134,  139628, 19062, 10487
  28,  170577,  139629, 20522, 13441
  29,  160213,  139628, 23497, 10034
  30,  161786,  139628, 35519, 17630
  31,  168673,  139629, 26396, 16985
  32,  166058,  139628, 23296, 12905
  33,  167890,  139628, 18289, 11421
  34,  162089,  139628, 39979, 19397
  35,  167862,  139629, 16066,  9676
  36,  162876,  139628, 23365, 10678
  37,  166153,  139628, 20585, 11178
  38,  165826,  139628, 20817, 11827
  39,  165925,  139629, 25960, 14123
  40,  162582,  139628, 24778, 11632
  41,  166056,  139628, 21727, 11979
  42,  163156,  139629, 26600, 13354
  43,  163301,  139628, 24998, 13076
  44,  167619,  139628, 27587, 15614
  45,  161798,  139628, 21002,  9565
  46,  159953,  139629, 33502, 14667
  47,  164861,  139628, 23292, 12831
  48,  166030,  139628, 19709, 10461
  49,  162294,  139629, 20553, 10033
  50,  168459,  139628, 17526, 11029
  51,  164484,  139628, 23513, 11249
  52,  163399,  139628, 17269,  8449
  53,  163336,  139629, 19540,  9458
  54,  166767,  139628, 25898, 14256
  55,  166081,  139628, 21924, 12214
  56,  163585,  139629, 19071,  9061
  57,  167462,  139628, 15305,  8962
  58,  165673,  139628, 12008,  6727
  59,  166158,  139628, 21094, 11373
  60,  167193,  139629, 18858, 10923
  61,  167955,  139628,  9775,  6386
  62,  162945,  139628, 20355,  9025
  63,  163990,  139629, 18028,  8627
  64,  166373,  139628, 18233, 10556
  65,  168395,  139628, 19687, 12081
  66,  165590,  139628, 20531, 10866
  67,  161729,  139629, 26517, 12033
  68,  162740,  139628, 25104, 12385
  69,  164487,  139628, 30186, 15825
  70,  164441,  139628, 15962,  8671
  71,  163691,  139629, 25778, 12924
  72,  165577,  139628, 28639, 15051
  73,  165759,  139628, 18503, 10016
  74,  166579,  139629, 25314, 14258
  75,  164253,  139628, 25478, 13295
  76,  161221,  139628, 23164, 11247
  77,  163907,  139628, 22286, 11061
  78,  160354,  139629, 27142, 12187
  79,  168081,  139628, 25688, 14796
  80,  166563,  139628, 29158, 15070
  81,  163594,  139629, 18741,  9334
  82,  162483,  139628, 19944,  9827
  83,  165099,  139628, 24615, 13817
  84,  166510,  139628, 17566, 10099
  85,  167300,  139629, 20054, 12209
  86,  169589,  139628, 19277, 12017
  87,  161262,  139628, 28451, 12757
  88,  162144,  139629, 19116,  8611
  89,  164854,  139628, 20932, 10673
  90,  162391,  139628, 25756, 12026
  91,  168207,  139628, 16817, 10134
  92,  167264,  139629, 13374,  7684
  93,  161617,  139628, 20240,  9105
  94,  163510,  139628, 26908, 13437
  95,  164024,  139629, 17982,  8853
  96,  162062,  139628, 28678, 14300
  97,  165876,  139628, 22024, 11880
  98,  167558,  139628, 20128, 11739
  99,  168440,  139629, 17907, 11078
 100,  164432,  139628, 27280, 13534
 101,  161682,  139628, 32315, 14995
 102,  166593,  139628, 15894,  8913
 103,  161305,  139629, 25725, 11952
 104,  165517,  139628, 18484, 10487
 105,  164505,  139628, 27550, 13265
 106,  168219,  139629, 20118, 12149
 107,  166341,  139628, 23910, 12217
 108,  161082,  139628, 26206, 12146
 109,  169414,  139628, 17809, 11548
 110,  164181,  139629, 31774, 15948
 111,  160068,  139628, 28641, 12778
 112,  161605,  139628, 25257, 11487
 113,  173582,  139629, 25290, 17817
 114,  166480,  139628, 16606,  9596
 115,  159574,  139628, 25284, 10982
 116,  163483,  139628, 22716, 11513
 117,  164053,  139629, 25952, 12656
 118,  164445,  139628, 19710,  9450
 119,  162187,  139628, 22417, 10712
 120,  162345,  139629, 21446, 10822
 121,  166453,  139628, 18506,  9994
 122,  164416,  139628, 30028, 14631
 123,  162896,  139628, 16627,  7473
 124,  163341,  139629, 24142, 11719
 125,  165354,  139628, 18330,  9118
 126,  165914,  139628, 14674,  8044
 127,  170443,  139629,  9571,  7614
gather statistics ... done in 
 0.00565 secs. on process 0 [Note: only showing root's timings]

Reinit_1st_2nd ... 
Reinit_1st_2nd ... done in 
 1.01946 secs. on process 0 [Note: only showing root's timings]

Reinit_2nd_2nd ... 
Reinit_2nd_2nd ... done in 
 1.95639 secs. on process 0 [Note: only showing root's timings]

total time ... done in 
 4.43190 secs. on process 0 [Note: only showing root's timings]

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./reinitialization_3d.prof on a sandybridge-cxx named c415-701.stampede.tacc.utexas.edu with 128 processors, by mmirzade Sat Feb 15 13:57:01 2014
Using Petsc Release Version 3.4.3, Oct, 15, 2013 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.522e+00      1.00007   4.522e+00
Objects:              9.700e+01      1.00000   9.700e+01
Flops:                0.000e+00      0.00000   0.000e+00  0.000e+00
Flops/sec:            0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Messages:         8.482e+03     10.57606   2.956e+03  3.784e+05
MPI Message Lengths:  4.020e+07      2.83697   8.194e+03  3.101e+09
MPI Reductions:       1.300e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.5217e+00 100.0%  0.0000e+00   0.0%  3.784e+05 100.0%  8.194e+03      100.0%  1.290e+02  99.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

PetscBarrier          32 1.0 9.4735e-01 1.2 0.00e+00 0.0 2.3e+05 9.6e+03 0.0e+00 19  0 60 70  0  19  0 60 70  0     0
my_p4est_level_set::reinit_2nd_order                           1 1.0 1.9666e+00 1.0 0.00e+00 0.0 2.3e+05 8.4e+03 6.4e+01 43  0 62 63 49  43  0 62 63 50     0
my_p4est_level_set::reinit_1st_time_2nd_space                  1 1.0 1.0338e+00 1.0 0.00e+00 0.0 1.3e+05 7.8e+03 5.6e+01 23  0 36 34 43  23  0 36 34 43     0
my_p4est_level_set::reinit_1_iter_2nd_order                   60 1.0 2.0474e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 42  0  0  0  0  42  0  0  0  0     0
my_p4est_hierarchy_t::init                                     1 1.0 2.0978e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
my_p4est_node_neighbors_t::init                                1 1.0 1.5307e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
my_p4est_node_neighbors_t::2nd_derivatives_cent               32 1.0 9.4733e-01 1.2 0.00e+00 0.0 2.3e+05 9.6e+03 0.0e+00 19  0 60 70  0  19  0 60 70  0     0
my_p4est_nodes_new                                             1 1.0 6.3731e-01 1.0 0.00e+00 0.0 4.7e+03 1.6e+04 1.0e+00 14  0  1  2  1  14  0  1  2  1     0
my_p4est_new                                                   1 1.0 6.1211e-02 145.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
my_p4est_ghost_new                                             1 1.0 1.0410e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
my_p4est_refine                                                8 1.0 1.4864e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
my_p4est_partition                                             9 1.0 1.6313e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecScatterBegin      126 1.0 5.2153e-02 7.2 0.00e+00 0.0 3.0e+05 9.6e+03 0.0e+00  1  0 79 92  0   1  0 79 92  0     0
VecScatterEnd        126 1.0 3.7067e-0123.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    32             32     22923904     0
      Vector Scatter    16             16        16832     0
           Index Set    32             32      1191040     0
   IS L to G Mapping    16             16         9280     0
              Viewer     1              0            0     0
========================================================================================================================
Average time to get PetscTime(): 0
Average time for MPI_Barrier(): 1.38283e-05
Average time for zero size MPI_Send(): 3.3509e-06
#PETSc Option Table entries:
-enable-qnnn-buffer
-lmax 8
-lmin 0
-log_summary
-output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_128.N_8.2807901
-test strong
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Thu Dec 12 09:53:13 2013
Configure options: --with-x=0 -with-pic --with-external-packages-dir=/opt/apps/intel13/mvapich2_1_9/petsc/3.4/externalpackages --with-mpi-compilers=1 --with-mpi-dir=/opt/apps/intel13/mvapich2/1.9 --with-clanguage=C++ --with-scalar-type=real --with-dynamic-loading=0 --with-shared-libraries=1 --with-spai=1 --download-spai --with-hypre=1 --download-hypre --with-mumps=1 --download-mumps --with-scalapack=1 --download-scalapack --with-blacs=1 --download-blacs --with-spooles=1 --download-spooles --with-superlu=1 --download-superlu --with-superlu_dist=1 --download-superlu_dist --with-parmetis=1 --download-parmetis --with-metis=1 --download-metis --with-hdf5=1 --with-hdf5-dir=/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9 --with-debugging=no --with-blas-lapack-dir=/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 --with-mpiexec=mpirun_rsh --COPTFLAGS= --CXXOPTFLAGS= --FOPTFLAGS=
-----------------------------------------
Libraries compiled on Thu Dec 12 09:53:13 2013 on build.stampede.tacc.utexas.edu 
Machine characteristics: Linux-2.6.32-358.18.1.el6.x86_64-x86_64-with-centos-6.4-Final
Using PETSc directory: /opt/apps/intel13/mvapich2_1_9/petsc/3.4
Using PETSc arch: sandybridge-cxx
-----------------------------------------

Using C compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx  -wd1572    -fPIC   ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpif90  -fPIC    ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/include -I/opt/apps/intel13/mvapich2/1.9/include
-----------------------------------------

Using C linker: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx
Using Fortran linker: /opt/apps/intel13/mvapich2/1.9/bin/mpif90
Using libraries: -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lpetsc -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lsuperlu_4.3 -lHYPRE -lspai -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -lpthread -lparmetis -lmetis -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -L/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -lhdf5_fortran -lhdf5_hl -lhdf5 -Wl,-rpath,/opt/ofed/lib64 -L/opt/ofed/lib64 -Wl,-rpath,/opt/apps/limic2/0.5.5/lib -L/opt/apps/limic2/0.5.5/lib -Wl,-rpath,/opt/apps/intel13/mvapich2/1.9/lib -L/opt/apps/intel13/mvapich2/1.9/lib -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -lmpichf90 -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.2.146/compiler/lib/intel64 -lifport -lifcore -lm -lm -lmpichcxx -ldl -lmpich -lopa -lmpl -libmad -lrdmacm -libumad -libverbs -lrt -llimic2 -lpthread -limf -lsvml -lirng -lipgo -ldecimal -lcilkrts -lstdc++ -lgcc_s -lirc -lirc_s -ldl 
-----------------------------------------

##IPMv0.983####################################################################
# 
# command : ./reinitialization_3d.prof -lmin 0 -lmax 8 -test strong -output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_128.N_8.2807901 -log_summary -enable-qnnn-buffer  (completed)
# host    : c415-701/x86_64_Linux          mpi_tasks : 128 on 8 nodes
# start   : 02/15/14/13:56:56              wallclock : 4.611492 sec
# stop    : 02/15/14/13:57:01              %comm     : 14.88 
# gbytes  : 5.66474e+01 total              gflop/sec : NA 
#
##############################################################################
# region  : *       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                        128             1             1             1
# wallclock                  590.199       4.61093        4.6107       4.61149
# user                       609.485        4.7616       4.65229       4.82127
# system                     23.8883      0.186627      0.144977      0.281957
# mpi                        87.8178      0.686077      0.400543      0.896709
# %comm                                    14.8775       8.68658       19.4451
# gbytes                     56.6474      0.442558      0.435108      0.458721
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                 26.486        335404         30.16         4.49
# MPI_Probe                  22.1473          5620         25.22         3.75
# MPI_Allreduce              11.1561        146432         12.70         1.89
# MPI_Allgather              10.4734          6400         11.93         1.77
# MPI_Waitall                 6.4212         23168          7.31         1.09
# MPI_Recv                   6.40513          6256          7.29         1.09
# MPI_Isend                  1.78992         86985          2.04         0.30
# MPI_Start                  1.44058        297612          1.64         0.24
# MPI_Barrier               0.736752          1024          0.84         0.12
# MPI_Startall              0.297204         16128          0.34         0.05
# MPI_Bcast                 0.233524           768          0.27         0.04
# MPI_Gather                0.140571          2304          0.16         0.02
# MPI_Irecv                0.0585587         81365          0.07         0.01
# MPI_Recv_init            0.0171986         75584          0.02         0.00
# MPI_Send_init           0.00893807         75584          0.01         0.00
# MPI_Comm_rank           0.00192137         30720          0.00         0.00
# MPI_Comm_size           0.00172815         23808          0.00         0.00
# MPI_Send                0.00142789           636          0.00         0.00
# MPI_Wait               0.000384424           896          0.00         0.00
##############################################################################
# region  : reinit_2nd_2nd       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       2560            20            20            20
# wallclock                  171.563       1.34033       1.28826       1.41749
# user                       171.222       1.33767        1.2838       1.41779
# system                    0.195971    0.00153102             0      0.007999
# mpi                        9.09583     0.0710612    0.00269168      0.171028
# %comm                                    5.01319      0.204588       12.1774
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                8.61571         47240         94.72         5.02
# MPI_Waitall               0.304071          2560          3.34         0.18
# MPI_Start                  0.14625         47240          1.61         0.09
# MPI_Startall             0.0298009          2560          0.33         0.02
##############################################################################
# region  : 2nd_derivatives       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       4096            32            32            32
# wallclock                  112.715      0.880589      0.760303      0.946698
# user                       111.702      0.872672      0.755883      0.939857
# system                    0.926871    0.00724118             0      0.014998
# mpi                        15.6323      0.122128     0.0157832      0.250563
# %comm                                    12.9004       1.85507       26.8034
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                13.1427        226752         84.07        11.66
# MPI_Start                  1.21575        226752          7.78         1.08
# MPI_Waitall                1.02081         12288          6.53         0.91
# MPI_Startall              0.253039         12288          1.62         0.22
##############################################################################
# region  : reinit_1st_2nd       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       1280            10            10            10
# wallclock                  85.5657      0.668482      0.644732       0.70635
# user                        85.391      0.667117      0.643901      0.705894
# system                    0.085993    0.00067182             0         0.003
# mpi                        4.33213     0.0338448     0.0013691     0.0825252
# %comm                                    4.79151      0.208151       11.6833
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                4.09106         23620         94.44         4.78
# MPI_Waitall               0.148129          1280          3.42         0.17
# MPI_Start                0.0785783         23620          1.81         0.09
# MPI_Startall             0.0143637          1280          0.33         0.02
##############################################################################
# region  : ipm_noregion       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                      10752            84            84            84
# wallclock                  83.5028      0.652366      0.622686       0.68246
# user                       110.343      0.862056      0.765883      0.901862
# system                     18.6621      0.145798      0.115981      0.235965
# mpi                        15.2549      0.119179     0.0877725      0.155085
# %comm                                    17.4631       13.7965       23.7632
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce              6.78496        139392         44.48         8.13
# MPI_Allgather              5.94914          4096         39.00         7.12
# MPI_Barrier               0.736752          1024          4.83         0.88
# MPI_Waitany               0.636447         37792          4.17         0.76
# MPI_Waitall               0.362482          4096          2.38         0.43
# MPI_Recv                  0.362139           636          2.37         0.43
# MPI_Bcast                 0.233524           768          1.53         0.28
# MPI_Isend                 0.143424         75584          0.94         0.17
# MPI_Recv_init            0.0171986         75584          0.11         0.02
# MPI_Irecv                0.0149579         75584          0.10         0.02
# MPI_Send_init           0.00893807         75584          0.06         0.01
# MPI_Comm_rank           0.00186896         30464          0.01         0.00
# MPI_Comm_size           0.00163137         23552          0.01         0.00
# MPI_Send                0.00142789           636          0.01         0.00
##############################################################################
# region  : p4est_nodes_new       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                        128             1             1             1
# wallclock                  80.4741      0.628704      0.627787      0.637031
# user                        78.829      0.615852      0.609907      0.624905
# system                     1.54077     0.0120373      0.005999      0.016998
# mpi                        31.0392      0.242494      0.204416      0.268886
# %comm                                    38.0662       32.5614       42.3549
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                  22.1473          5620         71.35        27.52
# MPI_Recv                   6.04299          5620         19.47         7.51
# MPI_Allgather              2.61722           128          8.43         3.25
# MPI_Allreduce             0.162554           256          0.52         0.20
# MPI_Isend                0.0574808          5620          0.19         0.07
# MPI_Waitall              0.0110847           128          0.04         0.01
# MPI_Wait               0.000384424           896          0.00         0.00
# MPI_Comm_size          8.54153e-05           128          0.00         0.00
# MPI_Comm_rank           4.5043e-05           128          0.00         0.00
##############################################################################
# region  : p4est_partition       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       1152             9             9             9
# wallclock                  20.8605      0.162973      0.162788      0.163026
# user                         19.47       0.15211      0.145979      0.159976
# system                     1.28579     0.0100453         0.002      0.015998
# mpi                        2.58125      0.020166     0.0151908     0.0303736
# %comm                                    12.3698       9.31848       18.6416
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather              1.30286          1152         50.47         6.25
# MPI_Allreduce             0.922956          4608         35.76         4.42
# MPI_Waitall               0.195842          2304          7.59         0.94
# MPI_Gather                0.140571          2304          5.45         0.67
# MPI_Isend                0.0177534          1097          0.69         0.09
# MPI_Irecv               0.00126415          1097          0.05         0.01
##############################################################################
# region  : p4est_refine       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                       1024             8             8             8
# wallclock                  18.9969      0.148414      0.148207      0.148556
# user                       18.5612      0.145009      0.140977      0.148979
# system                    0.334955    0.00261684             0         0.005
# mpi                       0.864457    0.00675357    0.00425389    0.00964773
# %comm                                    4.54615       2.86651       6.49811
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather              0.60422          1024         69.90         3.18
# MPI_Allreduce             0.260237          2048         30.10         1.37
##############################################################################
# region  : p4est_ghost_new       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                        128             1             1             1
# wallclock                  13.2009      0.103132     0.0947745      0.104051
# user                       10.9343     0.0854245      0.046993      0.102984
# system                    0.825878    0.00645217             0      0.022996
# mpi                        5.99237     0.0468154      0.011322     0.0657092
# %comm                                    44.9927       10.8812       63.4852
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitall                4.37878           512         73.07        33.17
# MPI_Isend                  1.57126          4684         26.22        11.90
# MPI_Irecv                0.0423367          4684          0.71         0.32
##############################################################################
# region  : p4est_new       [ntasks] =    128
#
#                           [total]         <avg>           min           max 
# entries                        128             1             1             1
# wallclock                  3.05082     0.0238345   0.000239529     0.0610648
# user                       3.01754     0.0235745             0       0.06099
# system                    0.011998   9.37344e-05             0         0.001
# mpi                        3.02537     0.0236357   1.53333e-05     0.0608801
# %comm                                     38.706       6.40144       99.6976
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce              3.02536           128        100.00        99.17
# MPI_Comm_size          1.13593e-05           128          0.00         0.00
# MPI_Comm_rank          7.36536e-06           128          0.00         0.00
###############################################################################
 
TACC: Shutdown complete. Exiting.
