Sat Feb 15 13:55:39 CST 2014
TACC: Starting up job 2807876
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[WARNING]: option 'log_summary' does not exists in the database -- ignoring.
 -------------------== CASL Options Database ==------------------- 
 List of entered options:

  -enable-qnnn-buffer no-arg
  -lmax 8
  -lmin 0
  -output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_16.N_1.2807876
  -test strong
 ----------------------------------------------------------------- 
git commit hash value = c05f9ad (c05f9adbc735149569977450adba3bc430802ba5)
total time ... 
connectivity ... 
connectivity ... done in 
 0.00055 secs. on process 0 [Note: only showing root's timings]

p4est generation ... 
p4est generation ... done in 
 0.00134 secs. on process 0 [Note: only showing root's timings]

refine ... 
refine ... done in 
 2.48649 secs. on process 0 [Note: only showing root's timings]

partition ... 
partition ... done in 
 0.84796 secs. on process 0 [Note: only showing root's timings]

gather statistics ... 
% global_quads = 17872420 	 global_nodes = 21114960
% mpi_rank local_node_size local_quad_size ghost_node_size ghost_quad_size
   0, 1319963, 1117026, 30757, 16229
   1, 1324721, 1117026, 61300, 36386
   2, 1322362, 1117026, 51252, 28869
   3, 1324664, 1117027, 86180, 49662
   4, 1324679, 1117026, 74504, 42180
   5, 1309326, 1117026, 70845, 32572
   6, 1320850, 1117026, 43862, 23032
   7, 1324961, 1117027, 51963, 28761
   8, 1317446, 1117026, 66587, 35502
   9, 1315731, 1117026, 64011, 31954
  10, 1322400, 1117026, 45179, 26021
  11, 1314011, 1117027, 63025, 29029
  12, 1317948, 1117026, 63433, 32616
  13, 1319327, 1117026, 75708, 38213
  14, 1315409, 1117026, 68781, 34225
  15, 1321162, 1117027, 33315, 16447
gather statistics ... done in 
 0.00154 secs. on process 0 [Note: only showing root's timings]

Reinit_1st_2nd ... 
Reinit_1st_2nd ... done in 
 8.71213 secs. on process 0 [Note: only showing root's timings]

Reinit_2nd_2nd ... 
Reinit_2nd_2nd ... done in 
 13.99538 secs. on process 0 [Note: only showing root's timings]

total time ... done in 
 35.58825 secs. on process 0 [Note: only showing root's timings]

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./reinitialization_3d.prof on a sandybridge-cxx named c471-901.stampede.tacc.utexas.edu with 16 processors, by mmirzade Sat Feb 15 13:56:21 2014
Using Petsc Release Version 3.4.3, Oct, 15, 2013 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.568e+01      1.00000   3.568e+01
Objects:              9.700e+01      1.00000   9.700e+01
Flops:                0.000e+00      0.00000   0.000e+00  0.000e+00
Flops/sec:            0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Messages:         2.402e+03      3.58507   1.604e+03  2.566e+04
MPI Message Lengths:  1.022e+08      3.00950   4.066e+04  1.043e+09
MPI Reductions:       1.300e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.5678e+01 100.0%  0.0000e+00   0.0%  2.566e+04 100.0%  4.066e+04      100.0%  1.290e+02  99.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

PetscBarrier          32 1.0 6.6867e+00 1.1 0.00e+00 0.0 1.5e+04 4.8e+04 0.0e+00 18  0 60 70  0  18  0 60 70  0     0
my_p4est_level_set::reinit_2nd_order                           1 1.0 1.3997e+01 1.0 0.00e+00 0.0 1.6e+04 4.2e+04 6.4e+01 39  0 62 63 49  39  0 62 63 50     0
my_p4est_level_set::reinit_1st_time_2nd_space                  1 1.0 8.9367e+00 1.0 0.00e+00 0.0 9.1e+03 3.9e+04 5.6e+01 25  0 36 34 43  25  0 36 34 43     0
my_p4est_level_set::reinit_1_iter_2nd_order                   60 1.0 1.5384e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 42  0  0  0  0  42  0  0  0  0     0
my_p4est_hierarchy_t::init                                     1 1.0 1.9864e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
my_p4est_node_neighbors_t::init                                1 1.0 1.3821e+00 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
my_p4est_node_neighbors_t::2nd_derivatives_cent               32 1.0 6.6866e+00 1.1 0.00e+00 0.0 1.5e+04 4.8e+04 0.0e+00 18  0 60 70  0  18  0 60 70  0     0
my_p4est_nodes_new                                             1 1.0 6.7036e+00 1.0 0.00e+00 0.0 3.2e+02 7.5e+04 1.0e+00 19  0  1  2  1  19  0  1  2  1     0
my_p4est_new                                                   1 1.0 2.2759e-03 5.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
my_p4est_ghost_new                                             1 1.0 2.8007e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
my_p4est_refine                                                8 1.0 1.5453e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
my_p4est_partition                                             9 1.0 1.8001e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0
VecScatterBegin      126 1.0 7.6440e-02 3.3 0.00e+00 0.0 2.0e+04 4.8e+04 0.0e+00  0  0 79 92  0   0  0 79 92  0     0
VecScatterEnd        126 1.0 2.2404e+00 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    32             32    172940544     0
      Vector Scatter    16             16        16832     0
           Index Set    32             32      1992640     0
   IS L to G Mapping    16             16         9280     0
              Viewer     1              0            0     0
========================================================================================================================
Average time to get PetscTime(): 0
Average time for MPI_Barrier(): 2.81334e-06
Average time for zero size MPI_Send(): 4.18723e-06
#PETSc Option Table entries:
-enable-qnnn-buffer
-lmax 8
-lmin 0
-log_summary
-output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_16.N_1.2807876
-test strong
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Thu Dec 12 09:53:13 2013
Configure options: --with-x=0 -with-pic --with-external-packages-dir=/opt/apps/intel13/mvapich2_1_9/petsc/3.4/externalpackages --with-mpi-compilers=1 --with-mpi-dir=/opt/apps/intel13/mvapich2/1.9 --with-clanguage=C++ --with-scalar-type=real --with-dynamic-loading=0 --with-shared-libraries=1 --with-spai=1 --download-spai --with-hypre=1 --download-hypre --with-mumps=1 --download-mumps --with-scalapack=1 --download-scalapack --with-blacs=1 --download-blacs --with-spooles=1 --download-spooles --with-superlu=1 --download-superlu --with-superlu_dist=1 --download-superlu_dist --with-parmetis=1 --download-parmetis --with-metis=1 --download-metis --with-hdf5=1 --with-hdf5-dir=/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9 --with-debugging=no --with-blas-lapack-dir=/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 --with-mpiexec=mpirun_rsh --COPTFLAGS= --CXXOPTFLAGS= --FOPTFLAGS=
-----------------------------------------
Libraries compiled on Thu Dec 12 09:53:13 2013 on build.stampede.tacc.utexas.edu 
Machine characteristics: Linux-2.6.32-358.18.1.el6.x86_64-x86_64-with-centos-6.4-Final
Using PETSc directory: /opt/apps/intel13/mvapich2_1_9/petsc/3.4
Using PETSc arch: sandybridge-cxx
-----------------------------------------

Using C compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx  -wd1572    -fPIC   ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpif90  -fPIC    ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/include -I/opt/apps/intel13/mvapich2/1.9/include
-----------------------------------------

Using C linker: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx
Using Fortran linker: /opt/apps/intel13/mvapich2/1.9/bin/mpif90
Using libraries: -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lpetsc -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lsuperlu_4.3 -lHYPRE -lspai -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -lpthread -lparmetis -lmetis -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -L/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -lhdf5_fortran -lhdf5_hl -lhdf5 -Wl,-rpath,/opt/ofed/lib64 -L/opt/ofed/lib64 -Wl,-rpath,/opt/apps/limic2/0.5.5/lib -L/opt/apps/limic2/0.5.5/lib -Wl,-rpath,/opt/apps/intel13/mvapich2/1.9/lib -L/opt/apps/intel13/mvapich2/1.9/lib -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -lmpichf90 -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.2.146/compiler/lib/intel64 -lifport -lifcore -lm -lm -lmpichcxx -ldl -lmpich -lopa -lmpl -libmad -lrdmacm -libumad -libverbs -lrt -llimic2 -lpthread -limf -lsvml -lirng -lipgo -ldecimal -lcilkrts -lstdc++ -lgcc_s -lirc -lirc_s -ldl 
-----------------------------------------

##IPMv0.983####################################################################
# 
# command : ./reinitialization_3d.prof -lmin 0 -lmax 8 -test strong -output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_16.N_1.2807876 -log_summary -enable-qnnn-buffer  (completed)
# host    : c471-901/x86_64_Linux          mpi_tasks : 16 on 1 nodes
# start   : 02/15/14/13:55:46              wallclock : 35.713529 sec
# stop    : 02/15/14/13:56:21              %comm     : 18.07 
# gbytes  : 1.74231e+01 total              gflop/sec : NA 
#
##############################################################################
# region  : *       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                         16             1             1             1
# wallclock                  571.409        35.713       35.7129       35.7135
# user                       556.402       34.7752       33.6259       35.3526
# system                     10.4224        0.6514      0.598908      0.712891
# mpi                        103.241       6.45256       5.47659       7.27745
# %comm                                    18.0676        15.335       20.3776
# gbytes                     17.4231       1.08895       1.07016       1.12933
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                  35.1378           384         34.03         6.15
# MPI_Waitany                27.7811         22720         26.91         4.86
# MPI_Allreduce               15.333         18304         14.85         2.68
# MPI_Allgather               12.754           800         12.35         2.23
# MPI_Recv                   9.43153           460          9.14         1.65
# MPI_Waitall                2.32823          2896          2.26         0.41
# MPI_Start                 0.191116         20160          0.19         0.03
# MPI_Gather                0.121954           288          0.12         0.02
# MPI_Bcast                 0.105926            96          0.10         0.02
# MPI_Startall              0.023986          2016          0.02         0.00
# MPI_Isend                0.0181807          5936          0.02         0.00
# MPI_Irecv               0.00594869          5552          0.01         0.00
# MPI_Barrier             0.00553316           128          0.01         0.00
# MPI_Recv_init           0.00103026          5120          0.00         0.00
# MPI_Send_init          0.000750821          5120          0.00         0.00
# MPI_Comm_size           0.00039701          2976          0.00         0.00
# MPI_Comm_rank          0.000363645          3840          0.00         0.00
# MPI_Send               0.000207882            76          0.00         0.00
# MPI_Wait               6.21313e-05            64          0.00         0.00
##############################################################################
# region  : reinit_2nd_2nd       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                        320            20            20            20
# wallclock                  161.093       10.0683       10.0396       10.1745
# user                       160.902       10.0563       10.0245       10.1605
# system                    0.072987    0.00456169      0.001998      0.007998
# mpi                        2.58088      0.161305    0.00792365      0.299396
# %comm                                    1.58539     0.0789153       2.98215
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                2.56255          3200         99.29         1.59
# MPI_Start                0.0109846          3200          0.43         0.01
# MPI_Waitall             0.00510639           320          0.20         0.00
# MPI_Startall            0.00224473           320          0.09         0.00
##############################################################################
# region  : p4est_nodes_new       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                         16             1             1             1
# wallclock                  107.121       6.69505       6.68954       6.70099
# user                       102.123       6.38272       5.86511         6.603
# system                     1.61676      0.101047      0.085987      0.121981
# mpi                        50.3385       3.14616       2.76089       3.28857
# %comm                                    46.9506       41.2056       49.0759
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                  35.1378           384         69.80        32.80
# MPI_Recv                   9.42091           384         18.72         8.79
# MPI_Allgather              2.98555            16          5.93         2.79
# MPI_Allreduce              2.73957            32          5.44         2.56
# MPI_Waitall              0.0534302            16          0.11         0.05
# MPI_Isend               0.00122211           384          0.00         0.00
# MPI_Wait               6.21313e-05            64          0.00         0.00
# MPI_Comm_size          1.11349e-05            16          0.00         0.00
# MPI_Comm_rank           8.0131e-06            16          0.00         0.00
##############################################################################
# region  : 2nd_derivatives       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                        512            32            32            32
# wallclock                  100.336       6.27099       5.97947       6.68376
# user                       99.0549       6.19093       5.86311       6.65699
# system                    0.350949     0.0219343      0.008998      0.034994
# mpi                        13.5064       0.84415      0.418927       1.35669
# %comm                                    12.6299       6.88231       20.9658
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                12.8311         15360         95.00        12.79
# MPI_Waitall               0.479813          1536          3.55         0.48
# MPI_Start                 0.175242         15360          1.30         0.17
# MPI_Startall             0.0202031          1536          0.15         0.02
##############################################################################
# region  : reinit_1st_2nd       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                        160            10            10            10
# wallclock                  93.1967        5.8248       5.41807       6.12144
# user                       91.1491       5.69682        5.2772       6.03408
# system                    0.034995    0.00218719             0      0.007998
# mpi                        12.3516      0.771975      0.381399       1.17908
# %comm                                     12.611       6.43104       19.2615
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                12.2031          1600         98.80        13.09
# MPI_Waitall               0.142016           160          1.15         0.15
# MPI_Start               0.00488982          1600          0.04         0.01
# MPI_Startall            0.00153818           160          0.01         0.00
##############################################################################
# region  : ipm_noregion       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                       1344            84            84            84
# wallclock                  51.6118       3.22573       3.17342       3.25716
# user                       48.4636       3.02898       2.76458       3.14152
# system                     7.34387      0.458992      0.432933      0.503923
# mpi                        6.49946      0.406216      0.152445      0.496492
# %comm                                    12.4715       4.69001       15.4634
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather               4.8179           512         74.13         9.33
# MPI_Allreduce              1.35507         17424         20.85         2.63
# MPI_Waitany               0.184263          2560          2.84         0.36
# MPI_Bcast                 0.105926            96          1.63         0.21
# MPI_Isend                0.0137466          5120          0.21         0.03
# MPI_Recv                 0.0106197            76          0.16         0.02
# MPI_Barrier             0.00553316           128          0.09         0.01
# MPI_Waitall             0.00232252           512          0.04         0.00
# MPI_Irecv               0.00135115          5120          0.02         0.00
# MPI_Recv_init           0.00103026          5120          0.02         0.00
# MPI_Send_init          0.000750821          5120          0.01         0.00
# MPI_Comm_size          0.000384421          2944          0.01         0.00
# MPI_Comm_rank           0.00035468          3808          0.01         0.00
# MPI_Send               0.000207882            76          0.00         0.00
##############################################################################
# region  : p4est_partition       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                        144             9             9             9
# wallclock                  28.7777       1.79861       1.78854        1.7993
# user                       27.3618       1.71011       1.51277       1.77973
# system                    0.530923     0.0331827      0.016998      0.052992
# mpi                        10.6937      0.668356      0.488626      0.733196
# %comm                                    37.1453       27.1565       40.7492
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce              9.38816           576         87.79        32.62
# MPI_Allgather              1.01579           144          9.50         3.53
# MPI_Waitall               0.167061           288          1.56         0.58
# MPI_Gather                0.121954           288          1.14         0.42
# MPI_Isend               0.00057741           112          0.01         0.00
# MPI_Irecv              0.000155221           112          0.00         0.00
##############################################################################
# region  : p4est_refine       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                        128             8             8             8
# wallclock                   24.553       1.53456       1.53376       1.54467
# user                       23.4074       1.46297       1.21082       1.51377
# system                    0.443928     0.0277455      0.018997      0.041993
# mpi                        5.77106      0.360691      0.113447      0.406447
# %comm                                    23.3507       7.34444       26.4958
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather              3.93471           128         68.18        16.03
# MPI_Allreduce              1.83634           256         31.82         7.48
##############################################################################
# region  : p4est_ghost_new       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                         16             1             1             1
# wallclock                  4.47008       0.27938        0.2787      0.279952
# user                        3.9274      0.245463      0.166974      0.254961
# system                    0.020996    0.00131225             0         0.003
# mpi                        1.48556     0.0928474   0.000951924      0.169869
# %comm                                    33.1655      0.340495       60.9505
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitall                1.47848            64         99.52        33.08
# MPI_Irecv               0.00444232           320          0.30         0.10
# MPI_Isend               0.00263459           320          0.18         0.06
##############################################################################
# region  : p4est_new       [ntasks] =     16
#
#                           [total]         <avg>           min           max 
# entries                         16             1             1             1
# wallclock                0.0200683    0.00125427   0.000216086    0.00206429
# user                      0.012999   0.000812437             0         0.002
# system                    0.005999   0.000374938             0         0.001
# mpi                      0.0138315    0.00086447   8.68831e-06    0.00155632
# %comm                                    41.8773       4.02076       77.4433
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce            0.0138291            16         99.98        68.91
# MPI_Comm_size          1.45379e-06            16          0.01         0.01
# MPI_Comm_rank          9.51812e-07            16          0.01         0.00
###############################################################################
 
TACC: Shutdown complete. Exiting.
