Sat Feb 15 13:55:54 CST 2014
TACC: Starting up job 2807884
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[WARNING]: option 'log_summary' does not exists in the database -- ignoring.
 -------------------== CASL Options Database ==------------------- 
 List of entered options:

  -enable-qnnn-buffer no-arg
  -lmax 8
  -lmin 0
  -output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_32.N_2.2807884
  -test strong
 ----------------------------------------------------------------- 
git commit hash value = c05f9ad (c05f9adbc735149569977450adba3bc430802ba5)
total time ... 
connectivity ... 
connectivity ... done in 
 0.00028 secs. on process 0 [Note: only showing root's timings]

p4est generation ... 
p4est generation ... done in 
 0.00766 secs. on process 0 [Note: only showing root's timings]

refine ... 
refine ... done in 
 0.92648 secs. on process 0 [Note: only showing root's timings]

partition ... 
partition ... done in 
 0.24565 secs. on process 0 [Note: only showing root's timings]

gather statistics ... 
% global_quads = 17872420 	 global_nodes = 21114960
% mpi_rank local_node_size local_quad_size ghost_node_size ghost_quad_size
   0,  660608,  558513, 11541,  6083
   1,  659355,  558513, 41630, 21728
   2,  661944,  558513, 34593, 20392
   3,  662777,  558513, 55963, 31550
   4,  663703,  558513, 33791, 20656
   5,  658659,  558513, 25505, 12333
   6,  663415,  558513, 45744, 26317
   7,  661249,  558514, 55155, 30848
   8,  663899,  558513, 44285, 25735
   9,  660780,  558513, 33665, 18189
  10,  655095,  558513, 43725, 21471
  11,  654231,  558513, 57081, 27076
  12,  661267,  558513, 39341, 20665
  13,  659583,  558513, 24200, 12637
  14,  662878,  558513, 23700, 13485
  15,  662083,  558514, 41119, 21787
  16,  662087,  558513, 45137, 24937
  17,  655359,  558513, 48693, 24575
  18,  662168,  558513, 46973, 25412
  19,  653563,  558513, 39702, 18296
  20,  657739,  558513, 45130, 23377
  21,  664661,  558513, 47025, 27414
  22,  657596,  558513, 35767, 16890
  23,  656415,  558514, 44359, 20972
  24,  663936,  558513, 34045, 20462
  25,  654012,  558513, 57113, 26565
  26,  664582,  558513, 32893, 18322
  27,  654745,  558513, 68943, 33588
  28,  661241,  558513, 45786, 25221
  29,  654168,  558513, 50583, 23296
  30,  656110,  558513, 44994, 21026
  31,  665052,  558514, 17071, 10565
gather statistics ... done in 
 0.00208 secs. on process 0 [Note: only showing root's timings]

Reinit_1st_2nd ... 
Reinit_1st_2nd ... done in 
 3.63451 secs. on process 0 [Note: only showing root's timings]

Reinit_2nd_2nd ... 
Reinit_2nd_2nd ... done in 
 7.09380 secs. on process 0 [Note: only showing root's timings]

total time ... done in 
 15.74473 secs. on process 0 [Note: only showing root's timings]

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./reinitialization_3d.prof on a sandybridge-cxx named c473-901.stampede.tacc.utexas.edu with 32 processors, by mmirzade Sat Feb 15 13:56:19 2014
Using Petsc Release Version 3.4.3, Oct, 15, 2013 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.583e+01      1.00001   1.583e+01
Objects:              9.700e+01      1.00000   9.700e+01
Flops:                0.000e+00      0.00000   0.000e+00  0.000e+00
Flops/sec:            0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Messages:         4.322e+03      5.38903   2.284e+03  7.308e+04
MPI Message Lengths:  6.916e+07      5.24041   1.976e+04  1.444e+09
MPI Reductions:       1.300e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.5826e+01 100.0%  0.0000e+00   0.0%  7.308e+04 100.0%  1.976e+04      100.0%  1.290e+02  99.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

PetscBarrier          32 1.0 3.0691e+00 1.1 0.00e+00 0.0 4.4e+04 2.3e+04 0.0e+00 19  0 60 70  0  19  0 60 70  0     0
my_p4est_level_set::reinit_2nd_order                           1 1.0 7.0959e+00 1.0 0.00e+00 0.0 4.5e+04 2.0e+04 6.4e+01 45  0 62 63 49  45  0 62 63 50     0
my_p4est_level_set::reinit_1st_time_2nd_space                  1 1.0 3.6518e+00 1.0 0.00e+00 0.0 2.6e+04 1.9e+04 5.6e+01 23  0 36 34 43  23  0 36 34 43     0
my_p4est_level_set::reinit_1_iter_2nd_order                   60 1.0 7.6027e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 47  0  0  0  0  47  0  0  0  0     0
my_p4est_hierarchy_t::init                                     1 1.0 7.7437e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
my_p4est_node_neighbors_t::init                                1 1.0 5.9298e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
my_p4est_node_neighbors_t::2nd_derivatives_cent               32 1.0 3.0690e+00 1.1 0.00e+00 0.0 4.4e+04 2.3e+04 0.0e+00 19  0 60 70  0  19  0 60 70  0     0
my_p4est_nodes_new                                             1 1.0 2.5096e+00 1.0 0.00e+00 0.0 9.1e+02 3.7e+04 1.0e+00 16  0  1  2  1  16  0  1  2  1     0
my_p4est_new                                                   1 1.0 2.3318e-02 71.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
my_p4est_ghost_new                                             1 1.0 1.3890e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
my_p4est_refine                                                8 1.0 5.8680e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
my_p4est_partition                                             9 1.0 5.8566e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecScatterBegin      126 1.0 5.9795e-02 6.8 0.00e+00 0.0 5.7e+04 2.3e+04 0.0e+00  0  0 79 92  0   0  0 79 92  0     0
VecScatterEnd        126 1.0 5.3300e-0118.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    32             32     86083456     0
      Vector Scatter    16             16        16832     0
           Index Set    32             32       762816     0
   IS L to G Mapping    16             16         9280     0
              Viewer     1              0            0     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 4.76837e-06
Average time for zero size MPI_Send(): 3.65824e-06
#PETSc Option Table entries:
-enable-qnnn-buffer
-lmax 8
-lmin 0
-log_summary
-output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_32.N_2.2807884
-test strong
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Thu Dec 12 09:53:13 2013
Configure options: --with-x=0 -with-pic --with-external-packages-dir=/opt/apps/intel13/mvapich2_1_9/petsc/3.4/externalpackages --with-mpi-compilers=1 --with-mpi-dir=/opt/apps/intel13/mvapich2/1.9 --with-clanguage=C++ --with-scalar-type=real --with-dynamic-loading=0 --with-shared-libraries=1 --with-spai=1 --download-spai --with-hypre=1 --download-hypre --with-mumps=1 --download-mumps --with-scalapack=1 --download-scalapack --with-blacs=1 --download-blacs --with-spooles=1 --download-spooles --with-superlu=1 --download-superlu --with-superlu_dist=1 --download-superlu_dist --with-parmetis=1 --download-parmetis --with-metis=1 --download-metis --with-hdf5=1 --with-hdf5-dir=/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9 --with-debugging=no --with-blas-lapack-dir=/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 --with-mpiexec=mpirun_rsh --COPTFLAGS= --CXXOPTFLAGS= --FOPTFLAGS=
-----------------------------------------
Libraries compiled on Thu Dec 12 09:53:13 2013 on build.stampede.tacc.utexas.edu 
Machine characteristics: Linux-2.6.32-358.18.1.el6.x86_64-x86_64-with-centos-6.4-Final
Using PETSc directory: /opt/apps/intel13/mvapich2_1_9/petsc/3.4
Using PETSc arch: sandybridge-cxx
-----------------------------------------

Using C compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx  -wd1572    -fPIC   ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpif90  -fPIC    ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/include -I/opt/apps/intel13/mvapich2/1.9/include
-----------------------------------------

Using C linker: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx
Using Fortran linker: /opt/apps/intel13/mvapich2/1.9/bin/mpif90
Using libraries: -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lpetsc -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lsuperlu_4.3 -lHYPRE -lspai -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -lpthread -lparmetis -lmetis -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -L/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -lhdf5_fortran -lhdf5_hl -lhdf5 -Wl,-rpath,/opt/ofed/lib64 -L/opt/ofed/lib64 -Wl,-rpath,/opt/apps/limic2/0.5.5/lib -L/opt/apps/limic2/0.5.5/lib -Wl,-rpath,/opt/apps/intel13/mvapich2/1.9/lib -L/opt/apps/intel13/mvapich2/1.9/lib -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -lmpichf90 -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.2.146/compiler/lib/intel64 -lifport -lifcore -lm -lm -lmpichcxx -ldl -lmpich -lopa -lmpl -libmad -lrdmacm -libumad -libverbs -lrt -llimic2 -lpthread -limf -lsvml -lirng -lipgo -ldecimal -lcilkrts -lstdc++ -lgcc_s -lirc -lirc_s -ldl 
-----------------------------------------

##IPMv0.983####################################################################
# 
# command : ./reinitialization_3d.prof -lmin 0 -lmax 8 -test strong -output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_32.N_2.2807884 -log_summary -enable-qnnn-buffer  (completed)
# host    : c473-901/x86_64_Linux          mpi_tasks : 32 on 2 nodes
# start   : 02/15/14/13:56:03              wallclock : 15.872911 sec
# stop    : 02/15/14/13:56:19              %comm     : 8.60 
# gbytes  : 2.34037e+01 total              gflop/sec : NA 
#
##############################################################################
# region  : *       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                         32             1             1             1
# wallclock                  507.916       15.8724       15.8723       15.8729
# user                       506.922       15.8413       15.7066       15.9096
# system                     12.6841      0.396377      0.336948      0.485926
# mpi                        43.6989       1.36559       1.01819       1.71708
# %comm                                    8.60327        6.4149       10.8181
# gbytes                     23.4037      0.731366      0.708202      0.752205
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                   20.437          1072         46.77         4.02
# MPI_Waitany                9.00333         64752         20.60         1.77
# MPI_Recv                   6.05422          1228         13.85         1.19
# MPI_Allgather              4.36628          1600          9.99         0.86
# MPI_Allreduce              1.48515         36608          3.40         0.29
# MPI_Waitall                1.11116          5792          2.54         0.22
# MPI_Bcast                 0.464505           192          1.06         0.09
# MPI_Start                 0.459996         57456          1.05         0.09
# MPI_Gather                0.114573           576          0.26         0.02
# MPI_Barrier              0.0658316           256          0.15         0.01
# MPI_Startall             0.0624701          4032          0.14         0.01
# MPI_Isend                0.0562954         16814          0.13         0.01
# MPI_Irecv                0.0115584         15742          0.03         0.00
# MPI_Recv_init           0.00279563         14592          0.01         0.00
# MPI_Send_init           0.00179422         14592          0.00         0.00
# MPI_Comm_size          0.000799494          5952          0.00         0.00
# MPI_Comm_rank          0.000657552          7680          0.00         0.00
# MPI_Send               0.000392855           156          0.00         0.00
# MPI_Wait               0.000112879           160          0.00         0.00
##############################################################################
# region  : reinit_2nd_2nd       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                        640            20            20            20
# wallclock                  161.782       5.05568       4.97656       5.10127
# user                       161.562       5.04883       4.97424       5.09323
# system                    0.092988    0.00290588             0      0.007998
# mpi                        2.94868     0.0921463    0.00373198      0.209026
# %comm                                    1.80634     0.0737958       4.11378
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                2.86089          9120         97.02         1.77
# MPI_Waitall              0.0446582           640          1.51         0.03
# MPI_Start                0.0369063          9120          1.25         0.02
# MPI_Startall            0.00622426           640          0.21         0.00
##############################################################################
# region  : 2nd_derivatives       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                       1024            32            32            32
# wallclock                  94.3242       2.94763        2.8825       3.06767
# user                       93.7857        2.9308       2.85856       3.05953
# system                    0.437936     0.0136855      0.002999      0.024996
# mpi                        5.14451      0.160766     0.0314195        0.4217
# %comm                                    5.24065       1.08046       13.7466
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                4.59202         43776         89.26         4.87
# MPI_Start                 0.404038         43776          7.85         0.43
# MPI_Waitall               0.095218          3072          1.85         0.10
# MPI_Startall             0.0532298          3072          1.03         0.06
##############################################################################
# region  : reinit_1st_2nd       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                        320            10            10            10
# wallclock                    80.94       2.52938       2.49855       2.55515
# user                       80.8277       2.52587       2.49762       2.54961
# system                     0.03799    0.00118719             0      0.003999
# mpi                        1.48362      0.046363    0.00322849      0.101732
# %comm                                    1.81449      0.127301       4.00605
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                1.44431          4560         97.35         1.78
# MPI_Start                 0.019052          4560          1.28         0.02
# MPI_Waitall              0.0172393           320          1.16         0.02
# MPI_Startall              0.003016           320          0.20         0.00
##############################################################################
# region  : p4est_nodes_new       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                         32             1             1             1
# wallclock                  80.1969       2.50615       2.50575        2.5086
# user                        78.642       2.45756       2.44663       2.47163
# system                     1.49378     0.0466805      0.032995      0.057991
# mpi                        28.9721      0.905379      0.832954      0.982894
# %comm                                    36.0911       33.2389       39.1811
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                   20.437          1072         70.54        25.48
# MPI_Recv                   6.02326          1072         20.79         7.51
# MPI_Allgather               2.4695            32          8.52         3.08
# MPI_Allreduce            0.0242573            64          0.08         0.03
# MPI_Waitall              0.0102747            32          0.04         0.01
# MPI_Isend               0.00771854          1072          0.03         0.01
# MPI_Wait               0.000112879           160          0.00         0.00
# MPI_Comm_size          2.27848e-05            32          0.00         0.00
# MPI_Comm_rank          1.54907e-05            32          0.00         0.00
##############################################################################
# region  : ipm_noregion       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                       2688            84            84            84
# wallclock                  48.2156       1.50674       1.49043       1.51861
# user                       51.1442       1.59826       1.51477       1.63275
# system                     9.42355      0.294486      0.261959      0.375942
# mpi                        2.31632     0.0723851     0.0440853      0.101434
# %comm                                    4.76653        2.9124       6.72501
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather             0.970691          1024         41.91         2.01
# MPI_Allreduce             0.603599         34848         26.06         1.25
# MPI_Bcast                 0.464505           192         20.05         0.96
# MPI_Waitany               0.106108          7296          4.58         0.22
# MPI_Barrier              0.0658316           256          2.84         0.14
# MPI_Isend                0.0399923         14592          1.73         0.08
# MPI_Recv                 0.0309597           156          1.34         0.06
# MPI_Waitall               0.025151          1024          1.09         0.05
# MPI_Irecv               0.00308752         14592          0.13         0.01
# MPI_Recv_init           0.00279563         14592          0.12         0.01
# MPI_Send_init           0.00179422         14592          0.08         0.00
# MPI_Comm_size          0.000773985          5888          0.03         0.00
# MPI_Comm_rank          0.000640427          7616          0.03         0.00
# MPI_Send               0.000392855           156          0.02         0.00
##############################################################################
# region  : p4est_refine       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                        256             8             8             8
# wallclock                  18.7673      0.586479      0.586393      0.586536
# user                       18.3422      0.573194      0.560915      0.579911
# system                    0.401935     0.0125605      0.005999      0.017997
# mpi                       0.585051     0.0182828    0.00967958      0.025144
# %comm                                    3.11709       1.65037       4.28724
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather             0.489697           256         83.70         2.61
# MPI_Allreduce             0.095354           512         16.30         0.51
##############################################################################
# region  : p4est_partition       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                        288             9             9             9
# wallclock                  18.7316      0.585364      0.585066      0.585384
# user                       17.9123      0.559759      0.543917      0.570914
# system                    0.765888      0.023934      0.011999      0.038994
# mpi                        1.12624     0.0351949     0.0152979     0.0527318
# %comm                                    6.01228       2.61331       9.00827
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce             0.447644          1152         39.75         2.39
# MPI_Allgather             0.436395           288         38.75         2.33
# MPI_Waitall               0.123546           576         10.97         0.66
# MPI_Gather                0.114573           576         10.17         0.61
# MPI_Isend               0.00374422           238          0.33         0.02
# MPI_Irecv                0.0003372           238          0.03         0.00
##############################################################################
# region  : p4est_ghost_new       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                         32             1             1             1
# wallclock                  4.42972      0.138429      0.135894      0.138838
# user                       4.38933      0.137167       0.13398      0.138979
# system                    0.024994   0.000781062             0      0.002999
# mpi                       0.808047     0.0252515   0.000647832     0.0800127
# %comm                                    18.1877      0.467035        57.832
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitall               0.795073           128         98.39        17.95
# MPI_Irecv               0.00813367           912          1.01         0.18
# MPI_Isend               0.00484033           912          0.60         0.11
##############################################################################
# region  : p4est_new       [ntasks] =     32
#
#                           [total]         <avg>           min           max 
# entries                         32             1             1             1
# wallclock                 0.320983     0.0100307   0.000204488     0.0231031
# user                      0.311954    0.00974856             0      0.023996
# system                       0.004      0.000125             0         0.001
# mpi                       0.314296    0.00982174   1.39568e-05     0.0228637
# %comm                                    42.5126       6.82525       99.0797
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce             0.314291            32        100.00        97.92
# MPI_Comm_size          2.72412e-06            32          0.00         0.00
# MPI_Comm_rank          1.63447e-06            32          0.00         0.00
###############################################################################
 
TACC: Shutdown complete. Exiting.
