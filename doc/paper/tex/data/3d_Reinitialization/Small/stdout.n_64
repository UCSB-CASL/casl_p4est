Sat Feb 15 13:56:31 CST 2014
TACC: Starting up job 2807896
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[WARNING]: option 'log_summary' does not exists in the database -- ignoring.
 -------------------== CASL Options Database ==------------------- 
 List of entered options:

  -enable-qnnn-buffer no-arg
  -lmax 8
  -lmin 0
  -output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_64.N_4.2807896
  -test strong
 ----------------------------------------------------------------- 
git commit hash value = c05f9ad (c05f9adbc735149569977450adba3bc430802ba5)
total time ... 
connectivity ... 
connectivity ... done in 
 0.00199 secs. on process 0 [Note: only showing root's timings]

p4est generation ... 
p4est generation ... done in 
 0.00102 secs. on process 0 [Note: only showing root's timings]

refine ... 
refine ... done in 
 0.47084 secs. on process 0 [Note: only showing root's timings]

partition ... 
partition ... done in 
 0.12503 secs. on process 0 [Note: only showing root's timings]

gather statistics ... 
% global_quads = 17872420 	 global_nodes = 21114960
% mpi_rank local_node_size local_quad_size ghost_node_size ghost_quad_size
   0,  325297,  279256, 19705,  8077
   1,  335311,  279257, 20486, 13716
   2,  331580,  279256, 30084, 15857
   3,  327775,  279257, 24644, 12463
   4,  327631,  279256, 25004, 12392
   5,  334313,  279257, 31413, 19156
   6,  332701,  279256, 33193, 19357
   7,  330076,  279257, 38293, 20252
   8,  330271,  279257, 39382, 21654
   9,  333432,  279256, 23968, 14422
  10,  328007,  279257, 24211, 11069
  11,  330652,  279256, 28743, 15551
  12,  333810,  279257, 39165, 22326
  13,  329605,  279256, 26398, 14144
  14,  330790,  279257, 38721, 20751
  15,  330459,  279257, 33895, 19470
  16,  333948,  279256, 23914, 15146
  17,  329951,  279257, 43988, 23039
  18,  329029,  279256, 31218, 15340
  19,  331751,  279257, 30231, 17307
  20,  328638,  279256, 29008, 14446
  21,  326457,  279257, 33142, 16537
  22,  329417,  279256, 39164, 20145
  23,  324814,  279257, 33166, 14789
  24,  328324,  279257, 31671, 15930
  25,  332943,  279256, 33542, 18486
  26,  326735,  279257, 25084, 12022
  27,  332848,  279256, 26858, 15142
  28,  331047,  279257, 15055,  7978
  29,  331831,  279256, 23327, 12995
  30,  335148,  279257, 16959, 11375
  31,  326935,  279257, 28448, 12586
  32,  334768,  279256, 23270, 15038
  33,  327319,  279257, 35382, 16986
  34,  327227,  279256, 37205, 18716
  35,  328132,  279257, 32214, 16620
  36,  331336,  279256, 30013, 16130
  37,  330832,  279257, 32125, 17082
  38,  325128,  279256, 28032, 13067
  39,  328435,  279257, 23548, 11308
  40,  330157,  279257, 34305, 17263
  41,  327582,  279256, 30127, 16149
  42,  333810,  279257, 24441, 15489
  43,  330851,  279256, 37872, 19662
  44,  326998,  279257, 31501, 14813
  45,  330598,  279256, 22823, 11683
  46,  328881,  279257, 29861, 14943
  47,  327534,  279257, 29884, 14221
  48,  327938,  279256, 35191, 18090
  49,  335998,  279257, 22771, 14848
  50,  326114,  279256, 48858, 23001
  51,  327898,  279257, 33736, 16876
  52,  330022,  279256, 35911, 18445
  53,  334560,  279257, 25151, 14627
  54,  330496,  279256, 18491,  9770
  55,  324249,  279257, 54878, 26036
  56,  335187,  279257, 27516, 17070
  57,  326054,  279256, 35185, 17097
  58,  327536,  279257, 35939, 17368
  59,  326632,  279256, 31046, 14305
  60,  328798,  279257, 23541, 12449
  61,  327312,  279256, 37833, 17498
  62,  328695,  279257, 30752, 14912
  63,  336357,  279257, 11225,  8928
gather statistics ... done in 
 0.00327 secs. on process 0 [Note: only showing root's timings]

Reinit_1st_2nd ... 
Reinit_1st_2nd ... done in 
 1.89845 secs. on process 0 [Note: only showing root's timings]

Reinit_2nd_2nd ... 
Reinit_2nd_2nd ... done in 
 3.69142 secs. on process 0 [Note: only showing root's timings]

total time ... done in 
 8.22828 secs. on process 0 [Note: only showing root's timings]

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./reinitialization_3d.prof on a sandybridge-cxx named c472-501.stampede.tacc.utexas.edu with 64 processors, by mmirzade Sat Feb 15 13:56:49 2014
Using Petsc Release Version 3.4.3, Oct, 15, 2013 

                         Max       Max/Min        Avg      Total 
Time (sec):           8.271e+00      1.00003   8.271e+00
Objects:              9.700e+01      1.00000   9.700e+01
Flops:                0.000e+00      0.00000   0.000e+00  0.000e+00
Flops/sec:            0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Messages:         6.562e+03     10.22118   2.644e+03  1.692e+05
MPI Message Lengths:  5.429e+07      3.34043   1.254e+04  2.122e+09
MPI Reductions:       1.300e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 8.2711e+00 100.0%  0.0000e+00   0.0%  1.692e+05 100.0%  1.254e+04      100.0%  1.290e+02  99.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

PetscBarrier          32 1.0 1.6945e+00 1.2 0.00e+00 0.0 1.0e+05 1.5e+04 0.0e+00 19  0 60 70  0  19  0 60 70  0     0
my_p4est_level_set::reinit_2nd_order                           1 1.0 3.6914e+00 1.0 0.00e+00 0.0 1.0e+05 1.3e+04 6.4e+01 45  0 62 63 49  45  0 62 63 50     0
my_p4est_level_set::reinit_1st_time_2nd_space                  1 1.0 1.9001e+00 1.0 0.00e+00 0.0 6.0e+04 1.2e+04 5.6e+01 23  0 36 34 43  23  0 36 34 43     0
my_p4est_level_set::reinit_1_iter_2nd_order                   60 1.0 3.8695e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 45  0  0  0  0  45  0  0  0  0     0
my_p4est_hierarchy_t::init                                     1 1.0 3.9585e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
my_p4est_node_neighbors_t::init                                1 1.0 2.9891e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
my_p4est_node_neighbors_t::2nd_derivatives_cent               32 1.0 1.6945e+00 1.2 0.00e+00 0.0 1.0e+05 1.5e+04 0.0e+00 19  0 60 70  0  19  0 60 70  0     0
my_p4est_nodes_new                                             1 1.0 1.3092e+00 1.0 0.00e+00 0.0 2.1e+03 2.4e+04 1.0e+00 16  0  1  2  1  16  0  1  2  1     0
my_p4est_new                                                   1 1.0 1.3129e-02 18.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
my_p4est_ghost_new                                             1 1.0 1.0567e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
my_p4est_refine                                                8 1.0 2.9280e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
my_p4est_partition                                             9 1.0 3.0333e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecScatterBegin      126 1.0 5.7012e-02 8.6 0.00e+00 0.0 1.3e+05 1.5e+04 0.0e+00  0  0 79 92  0   0  0 79 92  0     0
VecScatterEnd        126 1.0 4.8638e-01 9.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    32             32     44208640     0
      Vector Scatter    16             16        16832     0
           Index Set    32             32      1285312     0
   IS L to G Mapping    16             16         9280     0
              Viewer     1              0            0     0
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
Average time for MPI_Barrier(): 1.08242e-05
Average time for zero size MPI_Send(): 3.10689e-06
#PETSc Option Table entries:
-enable-qnnn-buffer
-lmax 8
-lmin 0
-log_summary
-output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_64.N_4.2807896
-test strong
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Thu Dec 12 09:53:13 2013
Configure options: --with-x=0 -with-pic --with-external-packages-dir=/opt/apps/intel13/mvapich2_1_9/petsc/3.4/externalpackages --with-mpi-compilers=1 --with-mpi-dir=/opt/apps/intel13/mvapich2/1.9 --with-clanguage=C++ --with-scalar-type=real --with-dynamic-loading=0 --with-shared-libraries=1 --with-spai=1 --download-spai --with-hypre=1 --download-hypre --with-mumps=1 --download-mumps --with-scalapack=1 --download-scalapack --with-blacs=1 --download-blacs --with-spooles=1 --download-spooles --with-superlu=1 --download-superlu --with-superlu_dist=1 --download-superlu_dist --with-parmetis=1 --download-parmetis --with-metis=1 --download-metis --with-hdf5=1 --with-hdf5-dir=/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9 --with-debugging=no --with-blas-lapack-dir=/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 --with-mpiexec=mpirun_rsh --COPTFLAGS= --CXXOPTFLAGS= --FOPTFLAGS=
-----------------------------------------
Libraries compiled on Thu Dec 12 09:53:13 2013 on build.stampede.tacc.utexas.edu 
Machine characteristics: Linux-2.6.32-358.18.1.el6.x86_64-x86_64-with-centos-6.4-Final
Using PETSc directory: /opt/apps/intel13/mvapich2_1_9/petsc/3.4
Using PETSc arch: sandybridge-cxx
-----------------------------------------

Using C compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx  -wd1572    -fPIC   ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /opt/apps/intel13/mvapich2/1.9/bin/mpif90  -fPIC    ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/include -I/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/include -I/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/include -I/opt/apps/intel13/mvapich2/1.9/include
-----------------------------------------

Using C linker: /opt/apps/intel13/mvapich2/1.9/bin/mpicxx
Using Fortran linker: /opt/apps/intel13/mvapich2/1.9/bin/mpif90
Using libraries: -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lpetsc -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -L/opt/apps/intel13/mvapich2_1_9/petsc/3.4/sandybridge-cxx/lib -lsuperlu_4.3 -lHYPRE -lspai -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -lpthread -lparmetis -lmetis -Wl,-rpath,/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -L/opt/apps/intel13/mvapich2_1_9/phdf5/1.8.9/lib -lhdf5_fortran -lhdf5_hl -lhdf5 -Wl,-rpath,/opt/ofed/lib64 -L/opt/ofed/lib64 -Wl,-rpath,/opt/apps/limic2/0.5.5/lib -L/opt/apps/limic2/0.5.5/lib -Wl,-rpath,/opt/apps/intel13/mvapich2/1.9/lib -L/opt/apps/intel13/mvapich2/1.9/lib -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -L/opt/apps/intel/13/composer_xe_2013.3.163/compiler/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -lmpichf90 -Wl,-rpath,/opt/apps/intel/13/composer_xe_2013.2.146/compiler/lib/intel64 -lifport -lifcore -lm -lm -lmpichcxx -ldl -lmpich -lopa -lmpl -libmad -lrdmacm -libumad -libverbs -lrt -llimic2 -lpthread -limf -lsvml -lirng -lipgo -ldecimal -lcilkrts -lstdc++ -lgcc_s -lirc -lirc_s -ldl 
-----------------------------------------

##IPMv0.983####################################################################
# 
# command : ./reinitialization_3d.prof -lmin 0 -lmax 8 -test strong -output-dir /scratch/02032/mmirzade/3d_reinit_s_low_buff/n_64.N_4.2807896 -log_summary -enable-qnnn-buffer  (completed)
# host    : c472-501/x86_64_Linux          mpi_tasks : 64 on 4 nodes
# start   : 02/15/14/13:56:41              wallclock : 8.314413 sec
# stop    : 02/15/14/13:56:49              %comm     : 10.29 
# gbytes  : 3.45346e+01 total              gflop/sec : NA 
#
##############################################################################
# region  : *       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                         64             1             1             1
# wallclock                  532.081       8.31376       8.31365       8.31441
# user                       541.644       8.46318       8.32973       8.51571
# system                     18.1272      0.283238      0.247962      0.377942
# mpi                        54.7472      0.855425      0.612308       1.16488
# %comm                                    10.2885       7.36505       14.0104
# gbytes                     34.5346      0.539603      0.521347      0.553444
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                  17.3922          2496         31.77         3.27
# MPI_Waitany                15.2061        149952         27.78         2.86
# MPI_Allgather              10.9427          3200         19.99         2.06
# MPI_Recv                   4.08449          2812          7.46         0.77
# MPI_Allreduce              3.28038         73216          5.99         0.62
# MPI_Waitall                2.16865         11584          3.96         0.41
# MPI_Start                 0.845494        133056          1.54         0.16
# MPI_Bcast                 0.351297           384          0.64         0.07
# MPI_Gather                0.143545          1152          0.26         0.03
# MPI_Startall              0.133775          8064          0.24         0.03
# MPI_Isend                 0.117331         38886          0.21         0.02
# MPI_Barrier              0.0361667           512          0.07         0.01
# MPI_Irecv                0.0302976         36390          0.06         0.01
# MPI_Recv_init           0.00715434         33792          0.01         0.00
# MPI_Send_init           0.00426125         33792          0.01         0.00
# MPI_Comm_size           0.00122228         11904          0.00         0.00
# MPI_Comm_rank           0.00120019         15360          0.00         0.00
# MPI_Send               0.000744923           316          0.00         0.00
# MPI_Wait               0.000204975           384          0.00         0.00
##############################################################################
# region  : reinit_2nd_2nd       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                       1280            20            20            20
# wallclock                   165.48       2.58563       2.51489       2.67242
# user                       165.193       2.58114       2.51262        2.6666
# system                    0.127981     0.0019997             0         0.005
# mpi                         4.8449     0.0757016    0.00613589      0.219343
# %comm                                    2.83269      0.239966       8.20765
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                4.69187         21120         96.84         2.84
# MPI_Start                0.0788811         21120          1.63         0.05
# MPI_Waitall               0.060665          1280          1.25         0.04
# MPI_Startall             0.0134889          1280          0.28         0.01
##############################################################################
# region  : 2nd_derivatives       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                       2048            32            32            32
# wallclock                  101.859       1.59155       1.47079       1.69369
# user                        101.11       1.57984       1.45978       1.68474
# system                    0.635909    0.00993608         0.002      0.017997
# mpi                         8.8036      0.137556     0.0363488      0.310445
# %comm                                    8.12171       2.29735       18.4763
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                7.73921        101376         87.91         7.60
# MPI_Start                 0.726493        101376          8.25         0.71
# MPI_Waitall               0.224232          6144          2.55         0.22
# MPI_Startall              0.113666          6144          1.29         0.11
##############################################################################
# region  : reinit_1st_2nd       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                        640            10            10            10
# wallclock                  82.2736       1.28553       1.25349       1.32338
# user                       82.1585       1.28373       1.25281        1.3188
# system                     0.05799   0.000906094             0         0.003
# mpi                        2.44546     0.0382103     0.0016061      0.106944
# %comm                                    2.88733      0.125867       8.08116
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitany                2.38364         10560         97.47         2.90
# MPI_Start                0.0401197         10560          1.64         0.05
# MPI_Waitall              0.0150786           640          0.62         0.02
# MPI_Startall            0.00662014           640          0.27         0.01
##############################################################################
# region  : p4est_nodes_new       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                         64             1             1             1
# wallclock                  83.2989       1.30155       1.30032       1.30865
# user                       81.3766       1.27151       1.26281        1.2828
# system                     1.84072     0.0287612      0.021997      0.035995
# mpi                        29.3502      0.458596      0.421495      0.510598
# %comm                                    35.0434       32.4145       39.0171
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Probe                  17.3922          2496         59.26        20.88
# MPI_Allgather              7.83239            64         26.69         9.40
# MPI_Recv                   3.98179          2496         13.57         4.78
# MPI_Allreduce            0.0994966           128          0.34         0.12
# MPI_Waitall              0.0227107            64          0.08         0.03
# MPI_Isend                0.0212555          2496          0.07         0.03
# MPI_Wait               0.000204975           384          0.00         0.00
# MPI_Comm_size          3.87393e-05            64          0.00         0.00
# MPI_Comm_rank          2.42246e-05            64          0.00         0.00
##############################################################################
# region  : ipm_noregion       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                       5376            84            84            84
# wallclock                  53.6305      0.837976      0.829085      0.848219
# user                       68.0556       1.06337      0.945856       1.12083
# system                     14.0468      0.219482      0.184972      0.307952
# mpi                        5.11003     0.0798442     0.0597187      0.104481
# %comm                                    9.41317       7.10263       12.4091
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather              2.11392          2048         41.37         3.94
# MPI_Allreduce              1.87856         69696         36.76         3.50
# MPI_Waitany               0.391381         16896          7.66         0.73
# MPI_Bcast                 0.351297           384          6.87         0.66
# MPI_Waitall               0.133925          2048          2.62         0.25
# MPI_Recv                  0.102701           316          2.01         0.19
# MPI_Isend                0.0796527         33792          1.56         0.15
# MPI_Barrier              0.0361667           512          0.71         0.07
# MPI_Irecv               0.00791827         33792          0.15         0.01
# MPI_Recv_init           0.00715434         33792          0.14         0.01
# MPI_Send_init           0.00426125         33792          0.08         0.01
# MPI_Comm_size           0.00117757         11776          0.02         0.00
# MPI_Comm_rank           0.00117192         15232          0.02         0.00
# MPI_Send               0.000744923           316          0.01         0.00
##############################################################################
# region  : p4est_partition       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                        576             9             9             9
# wallclock                  19.3999      0.303124      0.302946      0.303171
# user                       18.3552        0.2868      0.280956      0.295955
# system                     0.95485     0.0149195      0.006998      0.021997
# mpi                        1.49519     0.0233623     0.0117097     0.0381461
# %comm                                    7.70596       3.86279       12.5838
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce             0.638802          2304         42.72         3.29
# MPI_Allgather             0.559755           576         37.44         2.89
# MPI_Waitall               0.148084          1152          9.90         0.76
# MPI_Gather                0.143545          1152          9.60         0.74
# MPI_Isend                0.0043627           498          0.29         0.02
# MPI_Irecv              0.000637371           498          0.04         0.00
##############################################################################
# region  : p4est_refine       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                        512             8             8             8
# wallclock                  18.7233      0.292551      0.292339      0.292659
# user                       18.3122      0.286128      0.281956      0.291956
# system                    0.389942    0.00609284         0.001      0.010999
# mpi                       0.625714    0.00977679    0.00599052     0.0132875
# %comm                                    3.34067       2.04793       4.54136
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allgather             0.436601           512         69.78         2.33
# MPI_Allreduce             0.189114          1024         30.22         1.01
##############################################################################
# region  : p4est_ghost_new       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                         64             1             1             1
# wallclock                  6.68075      0.104387     0.0971737      0.105621
# user                         6.593      0.103016      0.094985      0.105983
# system                    0.046994   0.000734281             0         0.003
# mpi                        1.59775     0.0249649   0.000665701     0.0491239
# %comm                                    23.6363      0.630667       46.8545
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Waitall                1.56395           256         97.88        23.41
# MPI_Irecv                0.0217419          2100          1.36         0.33
# MPI_Isend                0.0120599          2100          0.75         0.18
##############################################################################
# region  : p4est_new       [ntasks] =     64
#
#                           [total]         <avg>           min           max 
# entries                         64             1             1             1
# wallclock                 0.507253    0.00792584   0.000511674     0.0128258
# user                      0.479927    0.00749886             0      0.012998
# system                    0.015999   0.000249984             0      0.001999
# mpi                        0.47442    0.00741281   1.44783e-05     0.0123454
# %comm                                    57.7962       2.49464       98.3842
#
#                            [time]       [calls]        <%mpi>      <%wall>
# MPI_Allreduce              0.47441            64        100.00        93.53
# MPI_Comm_size          5.96326e-06            64          0.00         0.00
# MPI_Comm_rank           4.0438e-06            64          0.00         0.00
###############################################################################
 
TACC: Shutdown complete. Exiting.
