%!TEX root = draft.tex
\section{Introduction}
\begin{enumerate}
\item \textbf{What is level-set and why is it important?}
The level-set method, originally proposed by Sethian and Osher \cite{Osher;Sethian:88:Fronts-Propagating-w}, is a popular and powerful framework for tracking arbitrary interfaces that undergo complicated topological changes. As a result, the level-set method has wide range of application such as multiphase flows, moving boundary problems, image segmentation, and computer graphics graphics, to only name a few \cite{Osher;Fedkiw:01:Level-Set-Methods:-A,Sethian:99:Level-set-methods-an}.

\item \textbf{What are the main issues of using level-set method?}
What makes level-set method powerful and easy to use is that in this method location of the interface is defined implicitly on an underlying grid. This convenience, however, comes at a price. First, compared to an explicit method, e.g. front tracking \fcite{front tracking refs}, level-set method is typically less accurate and mass conservation could be a problem although progress has been made in resolving this issue \cite{Enright;Fedkiw;Ferziger;etal:02:A-Hybrid-Particle-Le}\fcite{other?}. Second, the level-set function has to be defined in a higher dimensional space compared to the interface. If only the location of interface is needed, the added dimension greatly increases the overall computational cost. This problem, however, could be handled by computing the level-set only close to the interface, e.g. as in the narrow-band level-set method \cite{Brun;Guittet;Gibou:12:A-local-level-set-me}\fcite{narrow band ?}.

\item \textbf{Why using AMR helps? How do we plan to extend that paper here?}
Another approach that can address both problems is the use of local grid refinement. In \cite{Min;Gibou:07:A-second-order-accur}, authors present second-order accurate level-set methods on Cartesian Quadtree (two dimensions) and Octree (three dimensions) grids. Use of tree-based Cartesian adaptive grids in the context of level-set method is quite advantageous because 1) It gives fine-grain control over error, which typically occurs close to the interface, 2) It can effectively reduce the dimensionality of the problem by focusing most of the cells close to the interface, and 3) Construction and handling of the tree is quite simple in the presence of an interface which defines a an ideal metric for refinement. Although use of adaptive grids dramatically reduces the computational time, performing high-resolution three dimensional calculations of complex interfacial problems, e.g. crystal growth in binary alloys \fcite{Maxime's paper}, is still challenging. In this paper we extend these algorithms by proposing parallel implementations on distributed memory machines using domain decomposition technique and Message Passing Interface (MPI) library.

\item \textbf{What is the main challenge for parallelization?}
One of the main challenges in parallelizing level-set algorithms on adaptive grids, is handling the grid itself in parallel. One option is to replicate the entire grid on each processor and use off-the-self graph partitioners, e.g. Metis or ParMetis \fcite{metis/parmetis paper} or Zoltan \fcite{zoltan paper}, for load balancing and domain decomposition \fcite{origial deal.II paper}. This approach, however, is only scalable to a few hundred processor at best and is limited by the size of the grid itself. Moreover, use of a general-purpose graph partitioner adds extra overhead that limits the overall scalability even further. Interestingly, Quadtrees and Octrees have nice spatial ordering that naturally lead to the concept of Space Filling Curves (SFC) \fcite{SFC reference} which can be efficiently exploited for high quality load balancing.

\item \textbf{Parallel octrees: p4est, dendro, octor, etc. How do we plan to use p4est?}
The idea of using SFCs for parallel partitioning of Quad-/Octrees is not new in itself and has been tried by many researchers. For instance, Octor \fcite{octor} uses a Morton curve (also known as Z-curve) for traversing the leaves of an Octree which is then used to index and distribute them between processors and has been shown to scale up to 6200 cores \fcite{mantle convection octor}. Dendro \fcite{original dendro paper} is another example of parallel octree code in which similar ideas are used for parallel partitioning and development of a parallel geometric multigrid that has been scaled up to about 32000 cores \fcite{dendro parallel multigrid}. More recently, authors in \cite{Burstedde;Wilcox;Ghattas:11:p4est:-Scalable-Algo} extended these ideas to a collection, or a ``forest'', of octrees that are connected through a common, potentially unstructured, hexahedral grid. They then use a global Morton curve to partition the entire forest in parallel. Implementation of these algorithms, which were shown to scale to more than 200,000 processors, are publicly available through simple API provided by the \texttt{p4est} library \fcite{p4est website?}. In fact the algorithms presented in this paper are directly implemented on top of \texttt{p4est} library and we do not discuss any algorithm that is already covered in \texttt{p4est paper}. Instead we focus on parallelization of level-set method on Quad-/Octrees and make appropriate references to \cite{Burstedde;Wilcox;Ghattas:11:p4est:-Scalable-Algo} whenever necessary.

\item \textbf{Parallel advection methods? What are the challenges?}
Parallel level-set algorithms can be categorized in three separate groups: 1) Parallel advection algorithms, 2) Parallel reinitialization algorithms, and 3) Parallel motion under curvature algorithms. In this article we focus on developing algorithms for the first two categories. We also make use of the popular \texttt{PETSc} \fcite{petsc} library for solution of the linear systems that arise from the third category. Moreover, \texttt{PETSc} provides primitives, such as parallel ghosted vector and scatter/gather operations, which simplifies the implementation. 

Eulerian advection schemes can easily be parallelized but unfortunately are limited by the CFL condition. For adaptive grids, this could be very restrictive as the tree could be locally refined very deeply to capture quantities of interest. Semi-Lagrangian methods combine the unconditional stability of Lagrangian methods and the ease of use of Eulerian grids and have been successfully used for advecting the level-set function on Quad-/Octree grids before \cite{Min;Gibou:07:A-second-order-accur}. However, parallelizing the Semi-Lagrangian algorithm in a domain decomposition context is not an easy task. The reason for this is twofold: First, depending on the CFL number, the departure points may end up outside the ghost region and in remote processors that are potentially far away. This requires a very dynamic and nonuniform communication pattern which is complicated to implement. For adaptive grids situation is even more complicated due to the asymmetric nature of communication, i.e. every processor knows the message destination but no processor knows which processor to expect message from and how many. Second, depending on the CFL number, the distribution of departure points could be entirely independent of the original parallel partitioning. As a result large CFL numbers could result in very load-unbalanced distribution of departure points which restricts the scalability of the algorithm. Both of these problems, of course, could be avoided by choosing $\text{CFL} \le 1$ but that would defeat the purpose of using Semi-Lagrangian algorithm in the first place. 

\item \textbf{Parallel reinitialization methods? What are the challenges?}
In several applications of the level-set method, it is desirable for the level-set function to be a signed distance function, i.e. $|\nabla \phi| = 1$. There are generally two approaches to enforce this property: 1) Solving the pseudo-time transient reinitialization equation $\phi_\tau + S(\phi_0)\left(|\nabla \phi| - 1\right) = 0$ (c.f. section \ref{section::levelset method}) \cite{Osher;Fedkiw:01:Level-Set-Methods:-A} \fcite{smerka's paper?} or 2) Solving the static Eikonal equation $F(x)|\nabla\phi| = 1$ with constant speed function $F(x) \equiv 1$. The transient reinitialization equation can be solved using explicit finite differences and thus can easily be parallelized with a domain decomposition approach. Moreover, only a few iterations may be needed if the signed-distance property is only required close to the interface \cite{Min;Gibou:07:A-second-order-accur}. This is the approach we have chosen in this paper. If, however, the signed-distance property is required in the entire domain solving the static Eikonal equation is more computationally efficient. Unfortunately, however, the most popular algorithm for solving the Eikonal equation, i.e. the Fast Marching Method \cite{Sethian:96:A-Fast-Marching-Leve,Sethian:99:Level-set-methods-an}, is inherently sequential due to causal relationship between grid points and cannot be easily parallelized. The Fast Sweeping Method (FSM) \cite{Zhao:05:A-fast-sweeping-meth} is an alternative method for solving the static Eikonal equation iteratively which can be more computationally efficient for simple choices of speed function, e.g. as in this context, and for simple interfaces. Moreover, FSM has more potential for parallelization compared to the FMM.

\item \textbf{literature review on parallel levelset}
Parallel semi-Lagrangian algorithms have been of interest for atmospheric studies in the past. A simple domain decomposition technique was used in \cite{Thomas;Cote:95:Massively-parallel-s} where the width of ghost layer is fixed based on the maximum CFL number. At large CFL numbers, this technique potentially leads to large communication volume which can limit the scalability. Nonetheless good scaling was report for small CFL numbers ($\text{CFL} \le 2$). In \cite{Drake;Foster;Michalakes;etal:95:Design-and-performan} authors propose a more sophisticated domain decomposition approach which uses a ``dynamic ghost layer''. Here width of the ghost layer is dynamically determined at runtime based on information from previous time steps. Nonetheless this approach seems to suffer from excessive communication overhead at larger number of processors. More recently, authors in \cite{White-III;Dongarra:11:High-performance-hig} used a domain decomposition strategy on a cubed sphere but with a single layer of ghost nodes. Interpolation on remote processors is handled by sending query points to the corresponding processor and asking for the interpolated result. This approach seems to provide good scalability for transporting a single tracer up to about 1000 cores for $\text{CFL} \sim 10$. At higher CFL numbers the method begins to loose scalability due an increase in communication volume. Moreover this strategy is susceptible to load imbalances at higher CFL numbers and for non-uniform velocity fields. Although in this article we are mainly interested in parallel semi-Lagrangian methods, one could alternatively resort to finite difference or finite element discretization methods which are easier to parallelize. Indeed several algorithms has been proposed with applications in dendritic crystal growth \cite{Wang;Chang;Kale;etal:06:Parallelization-of-a}, multiphase flows \cite{Sussman:05:A-parallelized-adapt, Fortmeier;Bucker:11:A-parallel-strategy-, Rodriguez;Sahni;Lahey-Jr;etal:13:A-parallel-adaptive-}, atomization process \cite{Herrmann:10:A-parallel-Eulerian-}, and image segmentation on GPUs \cite{Lefohn;Cates;Whitaker:03:Interactive-GPU-base,Cates;Lefohn;Whitaker:04:GIST:-an-interactive,Roberts;Packer;Sousa;etal:10:A-work-efficient-GPU}.

As mentioned earlier, in this article we opt for the parallel implementation of the transient reinitialization equation which is considerably easier to parallelize. Nonetheless it is important to review some of the recent progress in parallelizing solvers for the static Eikonal equation. One of the earliest attempt in parallelizing the FMM is reported in \cite{Herrmann:03:A-domain-decompositi} where a domain decomposition algorithm was introduced. Unlike the serial FMM, however, parallel FMM potentially requires multiple iteration or ``roolback'' operations to enforce the causality property across processors. Similar ideas are described in details in \cite{Tugurlan:08:Fast-marching-method}. It should be noted that number of iterations for the parallel FMM to converge greatly depends on the complexity of the interface and also the parallel partitioning and, in general, fewer iterations are required if domains are more or less parallel to the interface normals. Due to the nature of Eikonal equation, shared memory machines might be a better environment for parallelization. For instance, in \cite{Breus;Cristiani;Gwosdek;etal:11:An-adaptive-domain-d} authors use an ``adaptive'' technique in which individual threads implicitly define a domain decomposition at runtime. Unfortunately, however, this approach does not seem to be more effective than a simple static domain decomposition and is not portable to a distributed memory machine. In \cite{Zhao:07:Parallel-implementat} a parallel FSM method was presented for the fist time. However, a more scalable FSM was more recently proposed in \cite{Detrixhe;Gibou;Min:13:A-parallel-fast-swee} where the Cuthill-McKee node numbering was utilize to expose more parallelism. More recently a two-scale, hybrid FMM-FSM was presented in \cite{Chacon;Vladimirsky:13:A-parallel-Heap-Cell} which, albeit being more complicated to implement, promises even better scalability. Finally, a parallel Fast Iterative Method (FIM) was proposed in \cite{Jeong;Whitaker:08:A-fast-iterative-met}. The FIM is similar to FMM in that it also maintains a list of ``active nodes''. Unlike FMM, however, FIM avoids sorting the list and allows for concurrent updating of all nodes in an iterative fashion.

\item \textbf{Whats the organization of the paper}
\end{enumerate}